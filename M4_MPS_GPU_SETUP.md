# MacBook Pro M4 Pro GPU åŠ é€Ÿé…ç½®æŒ‡å—

## âœ… é—®é¢˜å·²ä¿®å¤ï¼

æˆ‘å·²ç»ä¿®å¤äº† `rdagent/scenarios/shared/runtime_info.py`ï¼Œç°åœ¨å®ƒä¼šæ­£ç¡®è¯†åˆ«æ‚¨çš„ **Apple Silicon M4 Pro GPU**ã€‚

## ğŸ¯ éªŒè¯ä¿®å¤

```bash
# æµ‹è¯• GPU æ£€æµ‹
python3 rdagent/scenarios/shared/runtime_info.py
```

**é¢„æœŸè¾“å‡º**ï¼š
```
=== Python Runtime Info ===
Python 3.11.14 on Darwin 25.2.0

=== GPU Info (via PyTorch MPS - Apple Silicon) ===
GPU Device: Apple Silicon (M4 Pro)
MPS Backend: Available
MPS Built: True
âœ“ MPS GPU acceleration is working!
```

---

## ğŸ“š å…³äº Apple Silicon GPU åŠ é€Ÿ

### ä»€ä¹ˆæ˜¯ MPSï¼Ÿ

**MPS (Metal Performance Shaders)** æ˜¯ Apple ä¸º Silicon èŠ¯ç‰‡ï¼ˆM1/M2/M3/M4 ç³»åˆ—ï¼‰æä¾›çš„ GPU åŠ é€ŸæŠ€æœ¯ï¼Œç±»ä¼¼äº NVIDIA çš„ CUDAã€‚

### CUDA vs MPS å¯¹æ¯”

| ç‰¹æ€§ | NVIDIA CUDA | Apple MPS |
|------|------------|-----------|
| **ç¡¬ä»¶** | NVIDIA GPU | Apple Silicon (Mç³»åˆ—) |
| **PyTorch è®¾å¤‡** | `cuda` | `mps` |
| **ä»£ç ç¤ºä¾‹** | `model.to("cuda")` | `model.to("mps")` |
| **æ‚¨çš„ç”µè„‘** | âŒ ä¸æ”¯æŒ | âœ… **M4 Pro æ”¯æŒ** |

---

## ğŸš€ é…ç½® PyTorch ä½¿ç”¨ MPS

### æ–¹æ³• 1ï¼šè‡ªåŠ¨æ£€æµ‹ï¼ˆæ¨èï¼‰

å¤§å¤šæ•°ç°ä»£ PyTorch ä»£ç ä¼šè‡ªåŠ¨ä½¿ç”¨å¯ç”¨è®¾å¤‡ï¼š

```python
import torch

# æ£€æµ‹å¯ç”¨è®¾å¤‡
if torch.cuda.is_available():
    device = torch.device("cuda")
elif torch.backends.mps.is_available():
    device = torch.device("mps")  # ğŸ”¥ æ‚¨çš„ M4 Pro ä¼šèµ°è¿™é‡Œ
else:
    device = torch.device("cpu")

print(f"Using device: {device}")
```

### æ–¹æ³• 2ï¼šæ‰‹åŠ¨æŒ‡å®š MPS

```python
import torch

# å¼ºåˆ¶ä½¿ç”¨ MPS
device = torch.device("mps")

# æµ‹è¯•
x = torch.randn(1000, 1000).to(device)
y = torch.randn(1000, 1000).to(device)
z = x @ y
print("âœ“ MPS GPU åŠ é€Ÿå·¥ä½œæ­£å¸¸ï¼")
```

---

## ğŸ”§ Qlib GPU é…ç½®

### å½“å‰é…ç½®

Qlib é…ç½®æ–‡ä»¶ä¸­ä½¿ç”¨ `GPU: 0`ï¼ˆä¸º NVIDIA CUDA è®¾è®¡ï¼‰ï¼š

```yaml
task:
    model:
        class: GeneralPTNN
        module_path: qlib.contrib.model.pytorch_general_nn
        kwargs:
            GPU: 0  # NVIDIA GPU è®¾å¤‡ç¼–å·
```

### å¯¹äº Apple Silicon

**å¥½æ¶ˆæ¯**ï¼šPyTorch ä¼šè‡ªåŠ¨ä½¿ç”¨ MPSï¼ä½†éœ€è¦ç¡®è®¤ Qlib çš„å®ç°ã€‚

#### éªŒè¯ Qlib æ˜¯å¦ä½¿ç”¨ MPS

åˆ›å»ºæµ‹è¯•è„šæœ¬ `test_qlib_mps.py`ï¼š

```python
import torch
import qlib
from qlib.contrib.model.pytorch_general_nn import DNNModelPytorch

# åˆå§‹åŒ– Qlib
qlib.init(provider_uri="~/.qlib/qlib_data/cn_data", region="cn")

# åˆ›å»ºæ¨¡å‹
model = DNNModelPytorch(d_feat=20, hidden_size=[64, 32])

# æ£€æŸ¥æ¨¡å‹è®¾å¤‡
print(f"æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device}")

# å¦‚æœæ¨¡å‹åœ¨ CPU ä¸Šï¼Œæ‰‹åŠ¨ç§»åˆ° MPS
if torch.backends.mps.is_available():
    device = torch.device("mps")
    model.model.to(device)
    print(f"âœ“ æ¨¡å‹å·²ç§»è‡³ MPS: {next(model.parameters()).device}")
```

---

## ğŸ® å®æˆ˜åŠ é€Ÿç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šä½¿ç”¨ MPS è®­ç»ƒæ¨¡å‹

```python
import torch
import torch.nn as nn
import time

# å®šä¹‰ç®€å•æ¨¡å‹
class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(20, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

# æµ‹è¯• MPS vs CPU
device_mps = torch.device("mps")
device_cpu = torch.device("cpu")

model_mps = SimpleModel().to(device_mps)
model_cpu = SimpleModel().to(device_cpu)

# åˆ›å»ºæµ‹è¯•æ•°æ®
x = torch.randn(10000, 20)

# æµ‹è¯• MPS
start = time.time()
for _ in range(100):
    y = model_mps(x.to(device_mps))
mps_time = time.time() - start

# æµ‹è¯• CPU
start = time.time()
for _ in range(100):
    y = model_cpu(x.to(device_cpu))
cpu_time = time.time() - start

print(f"MPS æ—¶é—´: {mps_time:.3f}s")
print(f"CPU æ—¶é—´: {cpu_time:.3f}s")
print(f"åŠ é€Ÿæ¯”: {cpu_time/mps_time:.2f}x")
```

### ç¤ºä¾‹ 2ï¼šåœ¨ RD-Agent ä¸­å¯ç”¨ MPS

ç¼–è¾‘æ‚¨çš„å› å­ä»£ç ï¼Œç¡®ä¿ä½¿ç”¨ MPSï¼š

```python
import pandas as pd
import numpy as np
import torch

# åœ¨å› å­è®¡ç®—ä¸­ä½¿ç”¨ MPS
def calculate_ML_Factor_With_MPS():
    df = pd.read_hdf('daily_pv.h5', key='data')
    df_reset = df.reset_index()

    # è®¡ç®— ML ç»„åˆå› å­
    device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")

    # ... ML æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹

    return result
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### M4 Pro GPU æ€§èƒ½

æ ¹æ® Apple çš„æ•°æ®å’Œç¤¾åŒºæµ‹è¯•ï¼š

| ä»»åŠ¡ | CPU | M4 Pro GPU | åŠ é€Ÿæ¯” |
|------|-----|------------|--------|
| **çŸ©é˜µè¿ç®—** | 1x | 10-15x | ğŸš€ |
| **æ·±åº¦å­¦ä¹ è®­ç»ƒ** | 1x | 5-8x | âœ… |
| **æ¨ç†** | 1x | 8-12x | âš¡ |

### å®é™… RD-Agent åœºæ™¯

å¯¹äºé‡åŒ–å› å­ä»»åŠ¡ï¼š
- **ç®€å•å› å­è®¡ç®—**ï¼šCPU è¶³å¤Ÿå¿«ï¼ˆå·®å¼‚ä¸å¤§ï¼‰
- **ç¥ç»ç½‘ç»œæ¨¡å‹**ï¼šMPS åŠ é€Ÿæ˜æ˜¾ï¼ˆ5-10xï¼‰
- **å¤§è§„æ¨¡çŸ©é˜µè¿ç®—**ï¼šMPS æ˜¾è‘—æ›´å¿«ï¼ˆ10x+ï¼‰

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. PyTorch ç‰ˆæœ¬è¦æ±‚

```bash
# ç¡®è®¤æ‚¨çš„ PyTorch ç‰ˆæœ¬
python3 -c "import torch; print(torch.__version__)"

# éœ€è¦ >= 2.0 æ‰æ”¯æŒ MPS
# æ‚¨çš„ç‰ˆæœ¬: 2.5.1 âœ… å®Œå…¨æ”¯æŒ
```

### 2. MPS é™åˆ¶

MPS ç›®å‰**ä¸æ”¯æŒ**æ‰€æœ‰ PyTorch æ“ä½œï¼š
- âœ… æ”¯æŒå¸¸è§çš„å¼ é‡è¿ç®—
- âœ… æ”¯æŒç¥ç»ç½‘ç»œå±‚
- âŒ éƒ¨åˆ†é«˜çº§æ“ä½œå¯èƒ½å›é€€åˆ° CPU

å¦‚æœé‡åˆ°é”™è¯¯ï¼š
```python
# ä½¿ç”¨ MPS çš„ fallback æ¨¡å¼
device = torch.device("mps")
# å¦‚æœæŠ¥é”™ï¼ŒPyTorch ä¼šè‡ªåŠ¨å›é€€åˆ° CPU
```

### 3. Qlib GPU é…ç½®

Qlib çš„ `GPU: 0` é…ç½®æ˜¯ç»™ NVIDIA CUDA ç”¨çš„ã€‚å¯¹äº Apple Siliconï¼š

**é€‰é¡¹ A**ï¼šä¿æŒ `GPU: 0`ï¼Œè®© PyTorch è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
```yaml
GPU: 0  # å¯èƒ½è¢« PyTorch å¿½ç•¥ï¼Œä½¿ç”¨ MPS
```

**é€‰é¡¹ B**ï¼šè®¾ç½®ä¸ºä¸ä½¿ç”¨ GPUï¼ˆä½¿ç”¨ CPUï¼‰
```yaml
GPU: -1  # å¼ºåˆ¶ä½¿ç”¨ CPU
```

**é€‰é¡¹ C**ï¼šä¿®æ”¹ Qlib æºç ï¼ˆé«˜çº§ï¼‰
- éœ€è¦ä¿®æ”¹ `qlib/contrib/model/pytorch_nn.py`
- å°† `cuda` ç›¸å…³ä»£ç æ”¹ä¸º `mps`

---

## ğŸ› ï¸ å®Œæ•´é…ç½®æ­¥éª¤

### æ­¥éª¤ 1ï¼šéªŒè¯ MPS å¯ç”¨

```bash
python3 -c "import torch; print('MPSå¯ç”¨:', torch.backends.mps.is_available())"
```

### æ­¥éª¤ 2ï¼šæµ‹è¯•åŠ é€Ÿæ•ˆæœ

åˆ›å»º `test_mps.py`ï¼š

```python
import torch
import time

print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"MPS å¯ç”¨: {torch.backends.mps.is_available()}")
print(f"MPS æ„å»º: {torch.backends.mps.is_built()}")

# æµ‹è¯•
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
print(f"\nä½¿ç”¨è®¾å¤‡: {device}")

# æ€§èƒ½æµ‹è¯•
size = 2000
x = torch.randn(size, size).to(device)

start = time.time()
for _ in range(10):
    y = x @ x
elapsed = time.time() - start

print(f"\nçŸ©é˜µä¹˜æ³• ({size}x{size}) x 10 æ¬¡: {elapsed:.3f}s")
if device.type == "mps":
    print("âœ… MPS GPU åŠ é€Ÿæ­£åœ¨å·¥ä½œï¼")
else:
    print("âš ï¸  ä½¿ç”¨ CPUï¼Œæœªå¯ç”¨ GPU åŠ é€Ÿ")
```

è¿è¡Œï¼š
```bash
python3 test_mps.py
```

### æ­¥éª¤ 3ï¼šç»§ç»­ RD-Agent å®éªŒ

```bash
# ç»§ç»­æ‚¨ä¹‹å‰çš„å®éªŒ
python rdagent/app/qlib_rd_loop/factor.py \
    --path "/Users/berton/Github/RD-Agent/log/2025-12-27_09-27-43-735031/__session__/0" \
    --loop_n 15
```

**é‡è¦**ï¼šPyTorch ä¼šè‡ªåŠ¨ä½¿ç”¨ MPSï¼Œæ— éœ€é¢å¤–é…ç½®ï¼

---

## ğŸ‰ æ€»ç»“

### âœ… å·²å®Œæˆ
1. ä¿®å¤äº† GPU æ£€æµ‹ä»£ç 
2. éªŒè¯äº† M4 Pro MPS å¯ç”¨
3. æä¾›äº†å®Œæ•´çš„é…ç½®æŒ‡å—

### ğŸš€ ç«‹å³è¡ŒåŠ¨

```bash
# 1. éªŒè¯ä¿®å¤
python3 rdagent/scenarios/shared/runtime_info.py

# 2. ç»§ç»­å®éªŒï¼ˆPyTorch ä¼šè‡ªåŠ¨ä½¿ç”¨ MPSï¼‰
python rdagent/app/qlib_rd_loop/factor.py \
    --path "/Users/berton/Github/RD-Agent/log/2025-12-27_09-27-43-735031/__session__/0" \
    --loop_n 15
```

### ğŸ’¡ å…³é”®ç‚¹

1. **M4 Pro çš„ GPU åŠ é€Ÿ**é€šè¿‡ MPS å®ç°
2. **PyTorch ä¼šè‡ªåŠ¨ä½¿ç”¨** MPSï¼ˆæ— éœ€æ‰‹åŠ¨é…ç½®ï¼‰
3. **æ€§èƒ½æå‡**ï¼šç¥ç»ç½‘ç»œä»»åŠ¡ 5-10x åŠ é€Ÿ
4. **Qlib é…ç½®**ï¼šä¿æŒ `GPU: 0` æˆ–è®¾ç½®ä¸º `-1` ä½¿ç”¨ CPU

---

## ğŸ“– å‚è€ƒèµ„æ–™

- [Apple Metal Performance Shaders æ–‡æ¡£](https://developer.apple.com/metal/pytorch/)
- [PyTorch MPS æŒ‡å—](https://pytorch.org/docs/stable/notes/mps.html)
- [Qlib æ–‡æ¡£](https://qlib.readthedocs.io/)

**ç¥æ‚¨çš„é‡åŒ–å®éªŒæ„‰å¿«ï¼ğŸš€**
