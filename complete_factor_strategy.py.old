"""
å®Œæ•´ç‰ˆåŒå› å­é€‰è‚¡ç­–ç•¥
- ä½¿ç”¨å®Œæ•´ç‰ˆå› å­è®¡ç®—å…¬å¼ï¼ˆéç®€åŒ–ç‰ˆï¼‰
- æ”¯æŒç›´æ¥ä½¿ç”¨ $amount å­—æ®µï¼ˆå¦‚æœå¯ç”¨ï¼‰
- å‡½æ•°åŒ–è°ƒç”¨ï¼Œæ¨¡å—åŒ–è®¾è®¡
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# ==================== Qlib æ•°æ®è·å– ====================

def get_qlib_data(market='csi300', start_date='2023-01-01', end_date=None, use_amount=True):
    """
    ä» Qlib è·å–è‚¡ç¥¨æ•°æ®

    å‚æ•°:
        market: è‚¡ç¥¨æ±  ('csi300', 'csi500', 'all')
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ (é»˜è®¤ä¸ºä»Šå¤©)
        use_amount: æ˜¯å¦å°è¯•ä½¿ç”¨ $amount å­—æ®µ

    è¿”å›:
        DataFrame with columns: datetime, instrument, $open, $high, $low, $close, $volume, [$amount]
    """
    import qlib
    from qlib.data import D

    # åˆå§‹åŒ– Qlib
    qlib.init(provider_uri='~/.qlib/qlib_data/cn_data', region='cn')

    # è®¾ç½®ç»“æŸæ—¥æœŸ
    if end_date is None:
        end_date = datetime.now().strftime('%Y-%m-%d')

    # è·å–è‚¡ç¥¨æ± 
    instruments = D.instruments(market=market)
    print(f"âœ“ è‚¡ç¥¨æ± : {market.upper()}, å…± {len(instruments)} åªè‚¡ç¥¨")

    # åŸºç¡€å­—æ®µ
    base_fields = ['$open', '$high', '$low', '$close', '$volume']

    # å°è¯•ä½¿ç”¨ $amount å­—æ®µ
    if use_amount:
        try:
            # å…ˆå°è¯•è·å– $amount
            test_fields = base_fields + ['$amount']
            df_test = D.features(
                instruments=instruments[:10],  # å…ˆç”¨å°‘é‡è‚¡ç¥¨æµ‹è¯•
                fields=test_fields,
                start_time=start_date,
                end_time=end_date
            )
            if '$amount' in df_test.columns:
                print(f"âœ“ $amount å­—æ®µå¯ç”¨ï¼Œç›´æ¥ä½¿ç”¨")
                fields = base_fields + ['$amount']
            else:
                print(f"âœ— $amount å­—æ®µä¸å¯ç”¨ï¼Œå°†è‡ªè¡Œè®¡ç®—")
                fields = base_fields
                use_amount = False
        except:
            print(f"âœ— $amount å­—æ®µä¸å¯ç”¨ï¼Œå°†è‡ªè¡Œè®¡ç®—")
            fields = base_fields
            use_amount = False
    else:
        fields = base_fields

    # è·å–å®Œæ•´æ•°æ®
    df = D.features(
        instruments=instruments,
        fields=fields,
        start_time=start_date,
        end_time=end_date
    )
    df.columns = fields
    df = df.reset_index()

    # å¦‚æœ $amount ä¸å¯ç”¨ï¼Œè‡ªå·±è®¡ç®—
    if not use_amount and '$amount' not in df.columns:
        print("âœ“ è®¡ç®— $amount = $close Ã— $volume")
        df['$amount'] = df['$close'] * df['$volume']

    print(f"âœ“ æ•°æ®æ—¶é—´èŒƒå›´: {df['datetime'].min()} è‡³ {df['datetime'].max()}")
    print(f"âœ“ æ•°æ®é‡: {len(df)} è¡Œ")
    print(f"âœ“ æ•°æ®åˆ—: {df.columns.tolist()}")

    return df


# ==================== å› å­1: å®Œæ•´ç‰ˆæ»šåŠ¨å²­å›å½’å› å­ ====================

def calculate_feature_VAM_15(df):
    """
    ç‰¹å¾1: VAM_15 - 15æ—¥æ³¢åŠ¨ç‡è°ƒæ•´åŠ¨é‡
    å…¬å¼: VAM_{15} = M_{15} / Ïƒ_{15}
    å…¶ä¸­ M_{15} = (Close_t - Close_{t-15}) / Close_{t-15}
         Ïƒ_{15} = 15æ—¥æ”¶ç›Šç‡æ ‡å‡†å·®
    """
    print("  - è®¡ç®— VAM_15 (15æ—¥æ³¢åŠ¨ç‡è°ƒæ•´åŠ¨é‡)...")

    # è®¡ç®—æ—¥æ”¶ç›Šç‡
    df['daily_return'] = df.groupby('instrument')['$close'].transform(
        lambda x: x.pct_change(periods=1)
    )

    # è®¡ç®—15æ—¥åŠ¨é‡
    df['momentum_15'] = df.groupby('instrument')['$close'].transform(
        lambda x: x.pct_change(periods=15)
    )

    # è®¡ç®—15æ—¥æ”¶ç›Šç‡æ ‡å‡†å·®
    df['volatility_15'] = df.groupby('instrument')['daily_return'].transform(
        lambda x: x.rolling(window=15, min_periods=15).std()
    )

    # VAM_15 = åŠ¨é‡ / æ³¢åŠ¨ç‡
    df['VAM_15'] = df['momentum_15'] / df['volatility_15'].replace(0, np.nan)

    return df


def calculate_feature_VSVN_5_20(df):
    """
    ç‰¹å¾2: VSVN_5_20 - 5æ—¥æˆäº¤é‡æ¿€å˜å½’ä¸€åŒ–ï¼ˆç›¸å¯¹äº20æ—¥æˆäº¤é‡æ³¢åŠ¨ç‡ï¼‰
    å…¬å¼: VSVN_{5,20} = (Volume_t / MA_5(Volume_{t-5:t-1})) / Ïƒ_Volume,20
    å…¶ä¸­ MA_5 æ˜¯è¿‡å»5æ—¥å‡å€¼ï¼ˆæ’é™¤å½“å¤©ï¼‰
         Ïƒ_Volume,20 æ˜¯20æ—¥æˆäº¤é‡æ ‡å‡†å·®
    """
    print("  - è®¡ç®— VSVN_5_20 (æˆäº¤é‡æ¿€å˜å½’ä¸€åŒ–)...")

    # è®¡ç®—5æ—¥æˆäº¤é‡å‡å€¼ï¼ˆæ’é™¤å½“å¤©ï¼Œä½¿ç”¨shift(1)ï¼‰
    df['volume_ma_5'] = df.groupby('instrument')['$volume'].transform(
        lambda x: x.shift(1).rolling(window=5, min_periods=1).mean()
    )

    # æˆäº¤é‡æ¿€å˜ = å½“å‰æˆäº¤é‡ / 5æ—¥å‡å€¼
    df['volume_surge'] = df['$volume'] / df['volume_ma_5'].replace(0, np.nan)

    # è®¡ç®—20æ—¥æˆäº¤é‡æ ‡å‡†å·®
    df['volume_volatility_20'] = df.groupby('instrument')['$volume'].transform(
        lambda x: x.rolling(window=20, min_periods=20).std()
    )

    # VSVN_5_20 = æˆäº¤é‡æ¿€å˜ / æˆäº¤é‡æ³¢åŠ¨ç‡
    df['VSVN_5_20'] = df['volume_surge'] / df['volume_volatility_20'].replace(0, np.nan)

    return df


def calculate_feature_DDM_20(df):
    """
    ç‰¹å¾3: DDM_20 - 20æ—¥ä¸‹è¡Œåå·®è°ƒæ•´åŠ¨é‡
    å…¬å¼: DDM_{20} = M_{20} / DD_{20}
    å…¶ä¸­ M_{20} = (Close_t - Close_{t-20}) / Close_{t-20}
         DD_{20} = sqrt(1/20 Ã— Î£(min(R_i, 0))Â²), i=1 to 20
         R_i æ˜¯ç¬¬iå¤©çš„æ”¶ç›Šç‡
    """
    print("  - è®¡ç®— DDM_20 (20æ—¥ä¸‹è¡Œåå·®è°ƒæ•´åŠ¨é‡)...")

    # è®¡ç®—20æ—¥åŠ¨é‡
    df['momentum_20'] = df.groupby('instrument')['$close'].transform(
        lambda x: x.pct_change(periods=20)
    )

    # è®¡ç®—æ—¥æ”¶ç›Šç‡ï¼ˆç”¨äºä¸‹è¡Œåå·®ï¼‰
    df['daily_return'] = df.groupby('instrument')['$close'].transform(
        lambda x: x.pct_change()
    )

    # è®¡ç®—20æ—¥ä¸‹è¡Œåå·®
    # ä¸‹è¡Œåå·® = sqrt(å¹³å‡å€¼(è´Ÿæ”¶ç›Šç‡çš„å¹³æ–¹))
    def downside_deviation(returns):
        """è®¡ç®—ä¸‹è¡Œåå·®ï¼ˆåªè€ƒè™‘è´Ÿæ”¶ç›Šï¼‰"""
        negative_returns = returns[returns < 0]
        if len(negative_returns) == 0:
            return np.nan
        return np.sqrt((negative_returns ** 2).mean())

    df['downside_deviation_20'] = df.groupby('instrument')['daily_return'].transform(
        lambda x: x.rolling(window=20, min_periods=20).apply(downside_deviation)
    )

    # DDM_20 = åŠ¨é‡ / ä¸‹è¡Œåå·®
    df['DDM_20'] = df['momentum_20'] / df['downside_deviation_20'].replace(0, np.nan)

    return df


def calculate_feature_RSI_10(df):
    """
    ç‰¹å¾4: RSI_10 - 10æ—¥ç›¸å¯¹å¼ºå¼±æŒ‡æ ‡
    å…¬å¼: RSI_{10} = 100 - 100 / (1 + RS)
    å…¶ä¸­ RS = SMA(Gain, 10) / SMA(Loss, 10)
         Gain = max(Close_t - Close_{t-1}, 0)
         Loss = max(Close_{t-1} - Close_t, 0)
         SMA ä½¿ç”¨ç®€å•ç§»åŠ¨å¹³å‡
    """
    print("  - è®¡ç®— RSI_10 (10æ—¥ç›¸å¯¹å¼ºå¼±æŒ‡æ ‡)...")

    # è®¡ç®—ä»·æ ¼å˜åŒ–
    df['price_change'] = df.groupby('instrument')['$close'].transform(
        lambda x: x.diff()
    )

    # åˆ†ç¦»æ¶¨è·Œ
    df['gain'] = df['price_change'].apply(lambda x: x if x > 0 else 0)
    df['loss'] = df['price_change'].apply(lambda x: -x if x < 0 else 0)

    # è®¡ç®—å¹³å‡æ¶¨è·Œï¼ˆä½¿ç”¨SMAï¼‰
    df['avg_gain_10'] = df.groupby('instrument')['gain'].transform(
        lambda x: x.rolling(window=10, min_periods=10).mean()
    )
    df['avg_loss_10'] = df.groupby('instrument')['loss'].transform(
        lambda x: x.rolling(window=10, min_periods=10).mean()
    )

    # è®¡ç®—RS
    df['RS'] = df['avg_gain_10'] / df['avg_loss_10'].replace(0, np.nan)

    # è®¡ç®—RSI
    df['RSI_10'] = 100 - 100 / (1 + df['RS'])

    return df


def calculate_ridge_regression_factor(df, lambda_reg=0.1, window=60):
    """
    å®Œæ•´ç‰ˆæ»šåŠ¨å²­å›å½’å› å­

    å‚æ•°:
        df: åŒ…å« $open, $high, $low, $close, $volume çš„æ•°æ®
        lambda_reg: L2æ­£åˆ™åŒ–å‚æ•°ï¼Œé»˜è®¤0.1
        window: æ»šåŠ¨çª—å£å¤§å°ï¼Œé»˜è®¤60å¤©

    å…¬å¼:
        F_t = Î²_{0,t} + Î²_{1,t}Ã—VAM_{15,t} + Î²_{2,t}Ã—VSVN_{5,20,t} + Î²_{3,t}Ã—DDM_{20,t} + Î²_{4,t}Ã—RSI_{10,t}

        å…¶ä¸­ Î²_t é€šè¿‡å²­å›å½’ä¼°è®¡:
        Î²_t = (X'X + Î»I)^(-1) X'Y

        X æ˜¯ç‰¹å¾çŸ©é˜µ [VAM_15, VSVN_5_20, DDM_20, RSI_10]
        Y æ˜¯ç›®æ ‡å˜é‡ï¼ˆæ¬¡æ—¥æ”¶ç›Šç‡ï¼‰
        Î» æ˜¯æ­£åˆ™åŒ–å‚æ•°
        I æ˜¯å•ä½çŸ©é˜µ
    """
    print("\nğŸ§® è®¡ç®—å²­å›å½’å› å­...")

    df_reset = df.copy()
    df_reset = df_reset.sort_values(['instrument', 'datetime'])

    # è®¡ç®—å››ä¸ªç‰¹å¾
    df_reset = calculate_feature_VAM_15(df_reset)
    df_reset = calculate_feature_VSVN_5_20(df_reset)
    df_reset = calculate_feature_DDM_20(df_reset)
    df_reset = calculate_feature_RSI_10(df_reset)

    # è®¡ç®—ç›®æ ‡å˜é‡ï¼šæ¬¡æ—¥æ”¶ç›Šç‡
    df_reset['next_day_return'] = df_reset.groupby('instrument')['$close'].transform(
        lambda x: x.pct_change().shift(-1)
    )

    # ç‰¹å¾åˆ—è¡¨
    features = ['VAM_15', 'VSVN_5_20', 'DDM_20', 'RSI_10']

    print(f"  - æ‰§è¡Œæ»šåŠ¨å²­å›å½’ (çª—å£={window}å¤©, Î»={lambda_reg})...")

    def rolling_ridge_regression(group):
        """å¯¹å•ä¸ªè‚¡ç¥¨æ‰§è¡Œæ»šåŠ¨å²­å›å½’"""
        group = group.copy()
        group['Ridge_Factor'] = np.nan

        for i in range(window, len(group)):
            # è·å–å†å²æ•°æ®ï¼št-window åˆ° t-1
            X = group[features].iloc[i-window:i].values  # (window, 4)
            Y = group['next_day_return'].iloc[i-window:i].values  # (window,)

            # ç§»é™¤åŒ…å«NaNçš„è¡Œ
            valid_mask = ~np.isnan(X).any(axis=1) & ~np.isnan(Y)
            X_valid = X[valid_mask]
            Y_valid = Y[valid_mask]

            # è‡³å°‘éœ€è¦20ä¸ªæœ‰æ•ˆè§‚æµ‹å€¼
            if len(X_valid) >= 20:
                # æ·»åŠ æˆªè·é¡¹
                X_with_intercept = np.column_stack([np.ones(len(X_valid)), X_valid])

                try:
                    # å²­å›å½’: Î² = (X'X + Î»I)^(-1) X'Y
                    XtX = X_with_intercept.T @ X_with_intercept
                    reg_matrix = lambda_reg * np.eye(XtX.shape[0])

                    # è®¡ç®—å›å½’ç³»æ•°
                    beta = np.linalg.inv(XtX + reg_matrix) @ X_with_intercept.T @ Y_valid

                    # è®¡ç®—å½“å‰å› å­å€¼
                    current_features = group[features].iloc[i].values
                    current_with_intercept = np.concatenate([[1], current_features])

                    factor_value = current_with_intercept @ beta
                    group.iloc[i, group.columns.get_loc('Ridge_Factor')] = factor_value

                except np.linalg.LinAlgError:
                    # çŸ©é˜µå¥‡å¼‚ï¼Œè·³è¿‡
                    continue
                except Exception as e:
                    continue

        return group

    # å¯¹æ¯åªè‚¡ç¥¨åº”ç”¨æ»šåŠ¨å›å½’
    df_reset = df_reset.groupby('instrument', group_keys=False).apply(rolling_ridge_regression)

    return df_reset[['datetime', 'instrument', 'Ridge_Factor']]


# ==================== å› å­2: å®Œæ•´ç‰ˆXGBoostæ³¢åŠ¨ç‡åˆ¶åº¦å› å­ ====================

def calculate_xgboost_volatility_factor(df, max_depth=6, learning_rate=0.1, n_estimators=100):
    """
    å®Œæ•´ç‰ˆXGBoostæ³¢åŠ¨ç‡åˆ¶åº¦å› å­

    å‚æ•°:
        df: åŒ…å« $open, $high, $low, $close, $volume çš„æ•°æ®
        max_depth: XGBoostæ ‘çš„æœ€å¤§æ·±åº¦
        learning_rate: å­¦ä¹ ç‡
        n_estimators: æ ‘çš„æ•°é‡

    æ–¹æ³•:
        1. è®¡ç®—æ—¥å†…æ³¢åŠ¨ç‡ = High - Low
        2. è®¡ç®—è¿‡å»20æ—¥æ³¢åŠ¨ç‡ä¸­ä½æ•°ä½œä¸ºåŸºå‡†
        3. è®¡ç®—æœªæ¥5æ—¥å¹³å‡æ³¢åŠ¨ç‡ä½œä¸ºç›®æ ‡
        4. æ„é€ 30ä¸ªæ»åç‰¹å¾ï¼ˆä»·æ ¼ã€æˆäº¤é‡ã€åŠ¨é‡å„10ä¸ªæ»åï¼‰
        5. è®­ç»ƒXGBooståˆ†ç±»å™¨é¢„æµ‹é«˜/ä½æ³¢åŠ¨ç‡åˆ¶åº¦
        6. è¾“å‡ºé«˜æ³¢åŠ¨ç‡åˆ¶åº¦æ¦‚ç‡ä½œä¸ºå› å­å€¼
    """
    print("\nğŸ§® è®¡ç®—XGBoostæ³¢åŠ¨ç‡åˆ¶åº¦å› å­...")

    df_reset = df.copy()
    df_reset = df_reset.sort_values(['instrument', 'datetime'])

    # è®¡ç®—æ—¥å†…æ³¢åŠ¨ç‡å’ŒåŠ¨é‡
    df_reset['daily_range'] = df_reset['$high'] - df_reset['$low']
    df_reset['daily_momentum'] = df_reset.groupby('instrument')['$close'].transform(
        lambda x: x.pct_change()
    )

    # è®¡ç®—è¿‡å»20æ—¥æ³¢åŠ¨ç‡ä¸­ä½æ•°ï¼ˆåŸºå‡†ï¼‰
    df_reset['range_median_20d'] = df_reset.groupby('instrument')['daily_range'].transform(
        lambda x: x.rolling(window=20, min_periods=20).median()
    )

    # è®¡ç®—æœªæ¥5æ—¥å¹³å‡æ³¢åŠ¨ç‡ï¼ˆç›®æ ‡å˜é‡ï¼‰
    df_reset['range_avg_next_5d'] = df_reset.groupby('instrument')['daily_range'].transform(
        lambda x: x.shift(-5).rolling(window=5, min_periods=5).mean()
    )

    # ç›®æ ‡å˜é‡ï¼šäºŒåˆ†ç±»ï¼ˆ1=æœªæ¥5æ—¥é«˜æ³¢åŠ¨ï¼Œ0=æœªæ¥5æ—¥ä½æ³¢åŠ¨ï¼‰
    df_reset['target'] = (df_reset['range_avg_next_5d'] > df_reset['range_median_20d']).astype(int)

    # æ„é€ 30ä¸ªæ»åç‰¹å¾
    print("  - æ„é€ æ»åç‰¹å¾ (10ä¸ªæ»å Ã— 3ä¸ªå˜é‡ = 30ä¸ªç‰¹å¾)...")
    feature_cols = []

    for lag in range(1, 11):  # lag 1 to 10
        # ä»·æ ¼èŒƒå›´æ»å
        df_reset[f'range_lag_{lag}'] = df_reset.groupby('instrument')['daily_range'].transform(
            lambda x: x.shift(lag)
        )

        # æˆäº¤é‡æ»å
        df_reset[f'volume_lag_{lag}'] = df_reset.groupby('instrument')['$volume'].transform(
            lambda x: x.shift(lag)
        )

        # åŠ¨é‡æ»å
        df_reset[f'momentum_lag_{lag}'] = df_reset.groupby('instrument')['daily_momentum'].transform(
            lambda x: x.shift(lag)
        )

        feature_cols.extend([f'range_lag_{lag}', f'volume_lag_{lag}', f'momentum_lag_{lag}'])

    # ç§»é™¤åŒ…å«NaNçš„è¡Œ
    df_features = df_reset.dropna(subset=feature_cols + ['target']).copy()
    df_features = df_features.sort_values('datetime')

    if len(df_features) == 0:
        print("  âœ— æ²¡æœ‰æœ‰æ•ˆæ•°æ®ç”¨äºè®­ç»ƒ")
        return pd.DataFrame(columns=['datetime', 'instrument', 'XGBoost_Factor'])

    # æ—¶é—´åºåˆ—åˆ†å‰²ï¼ˆé¿å…å‰ç»åå·®ï¼‰
    unique_dates = sorted(df_features['datetime'].unique())
    split_idx = int(len(unique_dates) * 0.7)
    train_dates = set(unique_dates[:split_idx])

    train_mask = df_features['datetime'].isin(train_dates)

    X_train = df_features[train_mask][feature_cols]
    y_train = df_features[train_mask]['target']

    print(f"  - è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"  - æµ‹è¯•é›†: {len(df_features[~train_mask])} æ ·æœ¬")

    # è®­ç»ƒXGBooståˆ†ç±»å™¨
    print(f"  - è®­ç»ƒXGBoostæ¨¡å‹ (max_depth={max_depth}, n_estimators={n_estimators})...")
    model = xgb.XGBClassifier(
        max_depth=max_depth,
        learning_rate=learning_rate,
        n_estimators=n_estimators,
        random_state=42,
        verbosity=0
    )
    model.fit(X_train, y_train)

    # å¯¹æ‰€æœ‰æ•°æ®ç‚¹é¢„æµ‹
    print("  - é¢„æµ‹æ‰€æœ‰æ•°æ®ç‚¹...")
    df_all_features = df_reset.dropna(subset=feature_cols).copy()

    if len(df_all_features) > 0:
        X_all = df_all_features[feature_cols]

        # é¢„æµ‹æ¦‚ç‡ï¼ˆå±äºé«˜æ³¢åŠ¨ç‡åˆ¶åº¦çš„æ¦‚ç‡ï¼‰
        df_all_features['XGBoost_Factor'] = model.predict_proba(X_all)[:, 1]

        return df_all_features[['datetime', 'instrument', 'XGBoost_Factor']]

    return pd.DataFrame(columns=['datetime', 'instrument', 'XGBoost_Factor'])


# ==================== å› å­ç»„åˆä¸é€‰è‚¡ ====================

def combine_and_standardize_factors(ridge_df, xgb_df, ridge_weight=0.5, xgb_weight=0.5):
    """
    åˆå¹¶ä¸¤ä¸ªå› å­å¹¶è¿›è¡Œæ ‡å‡†åŒ–

    å‚æ•°:
        ridge_df: å²­å›å½’å› å­æ•°æ®
        xgb_df: XGBoostå› å­æ•°æ®
        ridge_weight: å²­å›å½’å› å­æƒé‡
        xgb_weight: XGBoostå› å­æƒé‡

    æ–¹æ³•:
        1. å†…è¿æ¥åˆå¹¶ä¸¤ä¸ªå› å­
        2. å¯¹æ¯ä¸ªæ—¥æœŸè¿›è¡Œæ¨ªæˆªé¢Z-scoreæ ‡å‡†åŒ–
        3. æŒ‰æƒé‡ç»„åˆ
    """
    print("\nğŸ“Š åˆå¹¶å’Œæ ‡å‡†åŒ–å› å­...")

    # åˆå¹¶æ•°æ®
    combined = pd.merge(
        ridge_df,
        xgb_df,
        on=['datetime', 'instrument'],
        how='inner'
    )

    print(f"  âœ“ åˆå¹¶åæ•°æ®é‡: {len(combined)} è¡Œ")

    # æ¨ªæˆªé¢Z-scoreæ ‡å‡†åŒ–
    combined['Ridge_zscore'] = combined.groupby('datetime')['Ridge_Factor'].transform(
        lambda x: (x - x.mean()) / x.std()
    )
    combined['XGBoost_zscore'] = combined.groupby('datetime')['XGBoost_Factor'].transform(
        lambda x: (x - x.mean()) / x.std()
    )

    # å¤„ç†æ— ç©·å€¼å’ŒNaN
    combined['Ridge_zscore'] = combined['Ridge_zscore'].replace([np.inf, -np.inf], np.nan).fillna(0)
    combined['XGBoost_zscore'] = combined['XGBoost_zscore'].replace([np.inf, -np.inf], np.nan).fillna(0)

    # æŒ‰æƒé‡ç»„åˆ
    combined['Combined_Factor'] = (
        combined['Ridge_zscore'] * ridge_weight +
        combined['XGBoost_zscore'] * xgb_weight
    )

    return combined


def select_stocks(combined_df, date, top_n=50, min_zscore=0.5):
    """
    é€‰è‚¡å‡½æ•°

    å‚æ•°:
        combined_df: åˆå¹¶å› å­æ•°æ®
        date: é€‰è‚¡æ—¥æœŸ
        top_n: é€‰å‡ºå‰Nåªè‚¡ç¥¨
        min_zscore: æœ€ä½å› å­å¾—åˆ†é˜ˆå€¼

    è¿”å›:
        stocks: é€‰ä¸­çš„è‚¡ç¥¨åˆ—è¡¨
        details: è¯¦ç»†ä¿¡æ¯DataFrame
    """
    # è·å–æŒ‡å®šæ—¥æœŸçš„æ•°æ®
    date_data = combined_df[combined_df['datetime'] == date].copy()

    if len(date_data) == 0:
        print(f"\nâœ— è­¦å‘Š: {date} æ²¡æœ‰æ•°æ®")
        return [], None

    # è¿‡æ»¤ï¼šå› å­å€¼å¿…é¡»å¤§äºé˜ˆå€¼
    filtered = date_data[date_data['Combined_Factor'] >= min_zscore]

    if len(filtered) == 0:
        # å¦‚æœæ²¡æœ‰è‚¡ç¥¨è¾¾åˆ°é˜ˆå€¼ï¼Œé€‰æ‹©top_n
        filtered = date_data.nlargest(top_n, 'Combined_Factor')
        print(f"\n  æ³¨æ„: æ²¡æœ‰è‚¡ç¥¨è¾¾åˆ°é˜ˆå€¼ {min_zscore}ï¼Œé€‰æ‹©top{top_n}")
    else:
        # ä»ç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨ä¸­é€‰æ‹©top_n
        filtered = filtered.nlargest(top_n, 'Combined_Factor')

    stocks = filtered['instrument'].tolist()

    print(f"\n{'='*70}")
    print(f"ğŸ“ˆ {date} é€‰è‚¡ç»“æœ")
    print(f"{'='*70}")
    print(f"  è‚¡ç¥¨æ± æ€»æ•°: {len(date_data)}")
    print(f"  ç¬¦åˆé˜ˆå€¼(>={min_zscore}): {len(date_data[date_data['Combined_Factor'] >= min_zscore])}")
    print(f"  æœ€ç»ˆé€‰ä¸­: {len(stocks)}åª")
    print(f"\n  å‰10åªè‚¡ç¥¨:")
    print(f"  {'æ’å':<6}{'ä»£ç ':<12}{'ç»¼åˆå¾—åˆ†':<10}{'å²­å›å½’':<10}{'XGBoost':<10}")
    print(f"  {'-'*60}")

    for i, (idx, row) in enumerate(filtered.head(10).iterrows(), 1):
        stock = row['instrument']
        score = row['Combined_Factor']
        ridge_score = row['Ridge_zscore']
        xgb_score = row['XGBoost_zscore']
        print(f"  {i:<6}{stock:<12}{score:>8.2f}    {ridge_score:>8.2f}    {xgb_score:>8.2f}")

    print(f"{'='*70}")

    return stocks, filtered


# ==================== ä¸»ç¨‹åº ====================

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    print("="*70)
    print("ğŸ¤– å®Œæ•´ç‰ˆåŒå› å­é€‰è‚¡ç­–ç•¥")
    print("="*70)

    # 1. è·å–æ•°æ®
    print("\nğŸ“¥ æ­¥éª¤ 1/4: è·å–Qlibæ•°æ®...")
    df = get_qlib_data(
        market='csi300',
        start_date='2024-01-01',
        end_date='2025-11-29',
        use_amount=True  # å°è¯•ä½¿ç”¨ $amount
    )

    # 2. è®¡ç®—å²­å›å½’å› å­
    print("\nğŸ“Š æ­¥éª¤ 2/4: è®¡ç®—å²­å›å½’å› å­...")
    ridge_factor = calculate_ridge_regression_factor(
        df,
        lambda_reg=0.1,  # L2æ­£åˆ™åŒ–å‚æ•°
        window=60         # æ»šåŠ¨çª—å£
    )
    print(f"  âœ“ å²­å›å½’å› å­è®¡ç®—å®Œæˆ")

    # 3. è®¡ç®—XGBoostå› å­
    print("\nğŸ“Š æ­¥éª¤ 3/4: è®¡ç®—XGBoostæ³¢åŠ¨ç‡åˆ¶åº¦å› å­...")
    xgb_factor = calculate_xgboost_volatility_factor(
        df,
        max_depth=6,
        learning_rate=0.1,
        n_estimators=100
    )
    print(f"  âœ“ XGBoostå› å­è®¡ç®—å®Œæˆ")

    # 4. åˆå¹¶å› å­
    print("\nğŸ“Š æ­¥éª¤ 4/4: åˆå¹¶å› å­å¹¶é€‰è‚¡...")
    combined = combine_and_standardize_factors(
        ridge_factor,
        xgb_factor,
        ridge_weight=0.5,
        xgb_weight=0.5
    )

    # ä¿å­˜å› å­æ•°æ®
    output_file = 'complete_factors.csv'
    combined.to_csv(output_file, index=False)
    print(f"\n  âœ“ å› å­æ•°æ®å·²ä¿å­˜: {output_file}")

    # é€‰è‚¡
    latest_date = combined['datetime'].max()
    stocks, details = select_stocks(
        combined,
        latest_date,
        top_n=50,
        min_zscore=0.5
    )

    # ä¿å­˜é€‰è‚¡ç»“æœ
    if details is not None:
        result_file = f'stock_selection_{latest_date.strftime("%Y%m%d")}.csv'
        details.to_csv(result_file, index=False, encoding='utf-8-sig')
        print(f"\n  âœ“ é€‰è‚¡ç»“æœå·²ä¿å­˜: {result_file}")

    print("\n" + "="*70)
    print("âœ… é€‰è‚¡å®Œæˆï¼")
    print("="*70)

    return combined, stocks, details


if __name__ == '__main__':
    try:
        combined, stocks, details = main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
