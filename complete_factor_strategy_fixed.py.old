"""
ä¿®å¤ç‰ˆå®Œæ•´åŒå› å­é€‰è‚¡ç­–ç•¥
- ä¿®å¤è‚¡ç¥¨æ± é—®é¢˜ï¼ˆç›´æ¥ä»æ–‡ä»¶è¯»å–ï¼‰
- ä¿®å¤ $amount è·å–æ–¹å¼
- ä¿®å¤æ—¥æœŸèŒƒå›´é—®é¢˜
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from datetime import datetime, timedelta
import os
import warnings
warnings.filterwarnings('ignore')


# ==================== Qlib æ•°æ®è·å–ï¼ˆä¿®å¤ç‰ˆï¼‰ ====================

def get_stock_list_from_file(market='csi300'):
    """
    ç›´æ¥ä»Qlibæ•°æ®æ–‡ä»¶è¯»å–è‚¡ç¥¨åˆ—è¡¨
    é¿å…D.instruments()çš„é—®é¢˜
    """
    qlib_data_path = os.path.expanduser('~/.qlib/qlib_data/cn_data/instruments')

    market_files = {
        'csi300': 'csi300.txt',
        'csi500': 'csi500.txt',
        'csi800': 'csi800.txt',
        'all': 'all.txt',
        'csiall': 'csiall.txt'
    }

    if market not in market_files:
        raise ValueError(f"Unknown market: {market}. Available: {list(market_files.keys())}")

    file_path = os.path.join(qlib_data_path, market_files[market])

    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Stock list file not found: {file_path}")

    # è¯»å–æ–‡ä»¶ï¼Œè·å–å½“å‰æœ‰æ•ˆçš„è‚¡ç¥¨
    stocks = []
    current_date = datetime.now().strftime('%Y-%m-%d')

    with open(file_path, 'r') as f:
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) >= 3:
                stock_code, start_date, end_date = parts[0], parts[1], parts[2]

                # æ£€æŸ¥æ˜¯å¦åœ¨æœ‰æ•ˆæœŸå†…
                if start_date <= current_date <= end_date:
                    stocks.append(stock_code)

    print(f"âœ“ ä»æ–‡ä»¶è¯»å–è‚¡ç¥¨æ± : {market.upper()}, å½“å‰æœ‰æ•ˆ {len(stocks)} åªè‚¡ç¥¨")

    return sorted(stocks)


def get_qlib_data_fixed(market='csi300', start_date='2024-01-01', end_date=None, use_amount=False):
    """
    ä¿®å¤ç‰ˆæ•°æ®è·å–å‡½æ•°

    ä¿®å¤:
        1. ç›´æ¥ä»æ–‡ä»¶è¯»å–è‚¡ç¥¨åˆ—è¡¨
        2. ä½¿ç”¨æ­£ç¡®çš„æ—¥æœŸæ ¼å¼
        3. $amount å­—æ®µå¤„ç†
    """
    import qlib
    from qlib.data import D

    # åˆå§‹åŒ– Qlib
    qlib.init(provider_uri='~/.qlib/qlib_data/cn_data', region='cn')

    # è®¾ç½®ç»“æŸæ—¥æœŸï¼ˆä½¿ç”¨æ˜¨å¤©çš„æ—¥æœŸï¼Œé¿å…æœªæ¥æ—¥æœŸï¼‰
    if end_date is None:
        end_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')

    # å¦‚æœæ˜¯æœªæ¥æ—¥æœŸï¼Œä¿®æ­£ä¸ºæ˜¨å¤©
    try:
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        if end_dt > datetime.now():
            end_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
            print(f"  âš ï¸  æ£€æµ‹åˆ°æœªæ¥æ—¥æœŸï¼Œå·²ä¿®æ­£ä¸º: {end_date}")
    except:
        pass

    # ç›´æ¥ä»æ–‡ä»¶è¯»å–è‚¡ç¥¨åˆ—è¡¨
    instruments = get_stock_list_from_file(market)

    # åŸºç¡€å­—æ®µ
    base_fields = ['$open', '$high', '$low', '$close', '$volume']

    # Qlibçš„amountå­—æ®µæ˜ å°„ï¼ˆå¯èƒ½éœ€è¦ä¸åŒçš„æ–¹å¼è·å–ï¼‰
    # æ ¹æ®æ–‡æ¡£ï¼Œå¯èƒ½éœ€è¦ç”¨ä¸åŒçš„è¡¨è¾¾å¼
    print(f"  âœ“ è·å–æ•°æ®æ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")

    # è·å–æ•°æ®ï¼ˆåˆ†æ‰¹å¤„ç†ï¼Œé¿å…å†…å­˜é—®é¢˜ï¼‰
    all_data = []
    batch_size = 100

    for i in range(0, len(instruments), batch_size):
        batch = instruments[i:i+batch_size]
        try:
            df_batch = D.features(
                instruments=batch,
                fields=base_fields,
                start_time=start_date,
                end_time=end_date
            )
            df_batch.columns = base_fields
            all_data.append(df_batch)
        except Exception as e:
            print(f"  âš ï¸  æ‰¹æ¬¡ {i//batch_size + 1} è·å–å¤±è´¥: {e}")
            continue

    if not all_data:
        raise ValueError("æ— æ³•è·å–ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥Qlibæ•°æ®æº")

    df = pd.concat(all_data, axis=0)
    df = df.reset_index()

    # è®¡ç®— $amountï¼ˆæˆäº¤é¢ï¼‰
    # æ³¨æ„ï¼šQlibçš„ $volume å•ä½é€šå¸¸æ˜¯"æ‰‹"ï¼ˆ100è‚¡ï¼‰
    # $amount = $close Ã— $volume Ã— 100
    print(f"  âœ“ è®¡ç®— $amount = $close Ã— $volume Ã— 100")
    df['$amount'] = df['$close'] * df['$volume'] * 100  # è½¬æ¢ä¸ºå…ƒ

    print(f"  âœ“ æ•°æ®æ—¶é—´èŒƒå›´: {df['datetime'].min()} è‡³ {df['datetime'].max()}")
    print(f"  âœ“ æ•°æ®é‡: {len(df)} è¡Œ")
    print(f"  âœ“ è‚¡ç¥¨æ•°é‡: {df['instrument'].nunique()}")

    return df


# ==================== å› å­1: å®Œæ•´ç‰ˆæ»šåŠ¨å²­å›å½’å› å­ ====================

def calculate_feature_VAM_15(df):
    """
    ç‰¹å¾1: VAM_15 - 15æ—¥æ³¢åŠ¨ç‡è°ƒæ•´åŠ¨é‡
    å…¬å¼: VAM_{15} = M_{15} / Ïƒ_{15}
    """
    print("  - è®¡ç®— VAM_15 (15æ—¥æ³¢åŠ¨ç‡è°ƒæ•´åŠ¨é‡)...")

    df['daily_return'] = df.groupby('instrument')['$close'].transform(
        lambda x: x.pct_change(periods=1)
    )
    df['momentum_15'] = df.groupby('instrument')['$close'].transform(
        lambda x: x.pct_change(periods=15)
    )
    df['volatility_15'] = df.groupby('instrument')['daily_return'].transform(
        lambda x: x.rolling(window=15, min_periods=15).std()
    )
    df['VAM_15'] = df['momentum_15'] / df['volatility_15'].replace(0, np.nan)

    return df


def calculate_feature_VSVN_5_20(df):
    """
    ç‰¹å¾2: VSVN_5_20 - 5æ—¥æˆäº¤é‡æ¿€å˜å½’ä¸€åŒ–
    å…¬å¼: VSVN_{5,20} = (Volume_t / MA_5(Volume_{t-5:t-1})) / Ïƒ_Volume,20
    """
    print("  - è®¡ç®— VSVN_5_20 (æˆäº¤é‡æ¿€å˜å½’ä¸€åŒ–)...")

    df['volume_ma_5'] = df.groupby('instrument')['$volume'].transform(
        lambda x: x.shift(1).rolling(window=5, min_periods=1).mean()
    )
    df['volume_surge'] = df['$volume'] / df['volume_ma_5'].replace(0, np.nan)
    df['volume_volatility_20'] = df.groupby('instrument')['$volume'].transform(
        lambda x: x.rolling(window=20, min_periods=20).std()
    )
    df['VSVN_5_20'] = df['volume_surge'] / df['volume_volatility_20'].replace(0, np.nan)

    return df


def calculate_feature_DDM_20(df):
    """
    ç‰¹å¾3: DDM_20 - 20æ—¥ä¸‹è¡Œåå·®è°ƒæ•´åŠ¨é‡
    å…¬å¼: DDM_{20} = M_{20} / DD_{20}
    """
    print("  - è®¡ç®— DDM_20 (20æ—¥ä¸‹è¡Œåå·®è°ƒæ•´åŠ¨é‡)...")

    df['momentum_20'] = df.groupby('instrument')['$close'].transform(
        lambda x: x.pct_change(periods=20)
    )
    df['daily_return'] = df.groupby('instrument')['$close'].transform(
        lambda x: x.pct_change()
    )

    def downside_deviation(returns):
        negative_returns = returns[returns < 0]
        if len(negative_returns) == 0:
            return np.nan
        return np.sqrt((negative_returns ** 2).mean())

    df['downside_deviation_20'] = df.groupby('instrument')['daily_return'].transform(
        lambda x: x.rolling(window=20, min_periods=20).apply(downside_deviation)
    )
    df['DDM_20'] = df['momentum_20'] / df['downside_deviation_20'].replace(0, np.nan)

    return df


def calculate_feature_RSI_10(df):
    """
    ç‰¹å¾4: RSI_10 - 10æ—¥ç›¸å¯¹å¼ºå¼±æŒ‡æ ‡
    å…¬å¼: RSI_{10} = 100 - 100 / (1 + RS)
    """
    print("  - è®¡ç®— RSI_10 (10æ—¥ç›¸å¯¹å¼ºå¼±æŒ‡æ ‡)...")

    df['price_change'] = df.groupby('instrument')['$close'].transform(lambda x: x.diff())
    df['gain'] = df['price_change'].apply(lambda x: x if x > 0 else 0)
    df['loss'] = df['price_change'].apply(lambda x: -x if x < 0 else 0)
    df['avg_gain_10'] = df.groupby('instrument')['gain'].transform(
        lambda x: x.rolling(window=10, min_periods=10).mean()
    )
    df['avg_loss_10'] = df.groupby('instrument')['loss'].transform(
        lambda x: x.rolling(window=10, min_periods=10).mean()
    )
    df['RS'] = df['avg_gain_10'] / df['avg_loss_10'].replace(0, np.nan)
    df['RSI_10'] = 100 - 100 / (1 + df['RS'])

    return df


def calculate_ridge_regression_factor(df, lambda_reg=0.1, window=60):
    """
    å®Œæ•´ç‰ˆæ»šåŠ¨å²­å›å½’å› å­
    """
    print("\nğŸ§® è®¡ç®—å²­å›å½’å› å­...")

    df_reset = df.copy()
    df_reset = df_reset.sort_values(['instrument', 'datetime'])

    # è®¡ç®—å››ä¸ªç‰¹å¾
    df_reset = calculate_feature_VAM_15(df_reset)
    df_reset = calculate_feature_VSVN_5_20(df_reset)
    df_reset = calculate_feature_DDM_20(df_reset)
    df_reset = calculate_feature_RSI_10(df_reset)

    # ç›®æ ‡å˜é‡
    df_reset['next_day_return'] = df_reset.groupby('instrument')['$close'].transform(
        lambda x: x.pct_change().shift(-1)
    )

    features = ['VAM_15', 'VSVN_5_20', 'DDM_20', 'RSI_10']
    print(f"  - æ‰§è¡Œæ»šåŠ¨å²­å›å½’ (çª—å£={window}å¤©, Î»={lambda_reg})...")

    def rolling_ridge_regression(group):
        group = group.copy()
        group['Ridge_Factor'] = np.nan

        for i in range(window, len(group)):
            X = group[features].iloc[i-window:i].values
            Y = group['next_day_return'].iloc[i-window:i].values

            valid_mask = ~np.isnan(X).any(axis=1) & ~np.isnan(Y)
            X_valid = X[valid_mask]
            Y_valid = Y[valid_mask]

            if len(X_valid) >= 20:
                X_with_intercept = np.column_stack([np.ones(len(X_valid)), X_valid])
                try:
                    XtX = X_with_intercept.T @ X_with_intercept
                    reg_matrix = lambda_reg * np.eye(XtX.shape[0])
                    beta = np.linalg.inv(XtX + reg_matrix) @ X_with_intercept.T @ Y_valid

                    current_features = group[features].iloc[i].values
                    current_with_intercept = np.concatenate([[1], current_features])
                    factor_value = current_with_intercept @ beta
                    group.iloc[i, group.columns.get_loc('Ridge_Factor')] = factor_value
                except:
                    continue

        return group

    df_reset = df_reset.groupby('instrument', group_keys=False).apply(rolling_ridge_regression)
    return df_reset[['datetime', 'instrument', 'Ridge_Factor']]


# ==================== å› å­2: å®Œæ•´ç‰ˆXGBoostæ³¢åŠ¨ç‡åˆ¶åº¦å› å­ ====================

def calculate_xgboost_volatility_factor(df, max_depth=6, learning_rate=0.1, n_estimators=100):
    """
    å®Œæ•´ç‰ˆXGBoostæ³¢åŠ¨ç‡åˆ¶åº¦å› å­
    """
    print("\nğŸ§® è®¡ç®—XGBoostæ³¢åŠ¨ç‡åˆ¶åº¦å› å­...")

    df_reset = df.copy()
    df_reset = df_reset.sort_values(['instrument', 'datetime'])

    df_reset['daily_range'] = df_reset['$high'] - df_reset['$low']
    df_reset['daily_momentum'] = df_reset.groupby('instrument')['$close'].transform(
        lambda x: x.pct_change()
    )

    df_reset['range_median_20d'] = df_reset.groupby('instrument')['daily_range'].transform(
        lambda x: x.rolling(window=20, min_periods=20).median()
    )

    df_reset['range_avg_next_5d'] = df_reset.groupby('instrument')['daily_range'].transform(
        lambda x: x.shift(-5).rolling(window=5, min_periods=5).mean()
    )

    df_reset['target'] = (df_reset['range_avg_next_5d'] > df_reset['range_median_20d']).astype(int)

    print("  - æ„é€ æ»åç‰¹å¾ (10ä¸ªæ»å Ã— 3ä¸ªå˜é‡ = 30ä¸ªç‰¹å¾)...")
    feature_cols = []

    for lag in range(1, 11):
        df_reset[f'range_lag_{lag}'] = df_reset.groupby('instrument')['daily_range'].transform(
            lambda x: x.shift(lag)
        )
        df_reset[f'volume_lag_{lag}'] = df_reset.groupby('instrument')['$volume'].transform(
            lambda x: x.shift(lag)
        )
        df_reset[f'momentum_lag_{lag}'] = df_reset.groupby('instrument')['daily_momentum'].transform(
            lambda x: x.shift(lag)
        )
        feature_cols.extend([f'range_lag_{lag}', f'volume_lag_{lag}', f'momentum_lag_{lag}'])

    df_features = df_reset.dropna(subset=feature_cols + ['target']).copy()
    df_features = df_features.sort_values('datetime')

    if len(df_features) == 0:
        print("  âœ— æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
        return pd.DataFrame(columns=['datetime', 'instrument', 'XGBoost_Factor'])

    unique_dates = sorted(df_features['datetime'].unique())
    split_idx = int(len(unique_dates) * 0.7)
    train_dates = set(unique_dates[:split_idx])

    train_mask = df_features['datetime'].isin(train_dates)
    X_train = df_features[train_mask][feature_cols]
    y_train = df_features[train_mask]['target']

    print(f"  - è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"  - è®­ç»ƒXGBoostæ¨¡å‹...")
    model = xgb.XGBClassifier(
        max_depth=max_depth,
        learning_rate=learning_rate,
        n_estimators=n_estimators,
        random_state=42,
        verbosity=0
    )
    model.fit(X_train, y_train)

    df_all_features = df_reset.dropna(subset=feature_cols).copy()
    if len(df_all_features) > 0:
        X_all = df_all_features[feature_cols]
        df_all_features['XGBoost_Factor'] = model.predict_proba(X_all)[:, 1]
        return df_all_features[['datetime', 'instrument', 'XGBoost_Factor']]

    return pd.DataFrame(columns=['datetime', 'instrument', 'XGBoost_Factor'])


# ==================== å› å­ç»„åˆä¸é€‰è‚¡ ====================

def combine_and_standardize_factors(ridge_df, xgb_df, ridge_weight=0.5, xgb_weight=0.5):
    """åˆå¹¶å¹¶æ ‡å‡†åŒ–å› å­"""
    print("\nğŸ“Š åˆå¹¶å’Œæ ‡å‡†åŒ–å› å­...")

    combined = pd.merge(ridge_df, xgb_df, on=['datetime', 'instrument'], how='inner')
    print(f"  âœ“ åˆå¹¶åæ•°æ®é‡: {len(combined)} è¡Œ")

    combined['Ridge_zscore'] = combined.groupby('datetime')['Ridge_Factor'].transform(
        lambda x: (x - x.mean()) / x.std()
    )
    combined['XGBoost_zscore'] = combined.groupby('datetime')['XGBoost_Factor'].transform(
        lambda x: (x - x.mean()) / x.std()
    )

    combined['Ridge_zscore'] = combined['Ridge_zscore'].replace([np.inf, -np.inf], np.nan).fillna(0)
    combined['XGBoost_zscore'] = combined['XGBoost_zscore'].replace([np.inf, -np.inf], np.nan).fillna(0)

    combined['Combined_Factor'] = (
        combined['Ridge_zscore'] * ridge_weight +
        combined['XGBoost_zscore'] * xgb_weight
    )

    return combined


def select_stocks(combined_df, date, top_n=50, min_zscore=0.5):
    """é€‰è‚¡"""
    date_data = combined_df[combined_df['datetime'] == date].copy()

    if len(date_data) == 0:
        print(f"\nâœ— {date} æ²¡æœ‰æ•°æ®")
        return [], None

    filtered = date_data[date_data['Combined_Factor'] >= min_zscore]

    if len(filtered) == 0:
        filtered = date_data.nlargest(top_n, 'Combined_Factor')
    else:
        filtered = filtered.nlargest(top_n, 'Combined_Factor')

    stocks = filtered['instrument'].tolist()

    print(f"\n{'='*70}")
    print(f"ğŸ“ˆ {date} é€‰è‚¡ç»“æœ (TOP {len(stocks)})")
    print(f"{'='*70}")
    print(f"  è‚¡ç¥¨æ± : {len(date_data)} åª | ç¬¦åˆé˜ˆå€¼: {len(date_data[date_data['Combined_Factor'] >= min_zscore])} åª")

    print(f"\n  å‰20åª:")
    for i, (idx, row) in enumerate(filtered.head(20).iterrows(), 1):
        print(f"  {i:2d}. {row['instrument']:10s} - ç»¼åˆå¾—åˆ†: {row['Combined_Factor']:6.2f} (å²­å›å½’: {row['Ridge_zscore']:5.2f}, XGBoost: {row['XGBoost_zscore']:5.2f})")

    return stocks, filtered


# ==================== ä¸»ç¨‹åº ====================

def main():
    print("="*70)
    print("ğŸ¤– å®Œæ•´ç‰ˆåŒå› å­é€‰è‚¡ç­–ç•¥ï¼ˆä¿®å¤ç‰ˆï¼‰")
    print("="*70)

    # 1. è·å–æ•°æ®ï¼ˆä¿®å¤æ—¥æœŸï¼‰
    print("\nğŸ“¥ æ­¥éª¤ 1/4: è·å–Qlibæ•°æ®...")
    df = get_qlib_data_fixed(
        market='csi300',
        start_date='2024-01-01',
        end_date='2024-12-31',  # ä¿®æ­£ä¸ºè¿‡å»çš„æ—¥æœŸ
        use_amount=False  # è‡ªå·±è®¡ç®—
    )

    # 2. è®¡ç®—å²­å›å½’å› å­
    print("\nğŸ“Š æ­¥éª¤ 2/4: è®¡ç®—å²­å›å½’å› å­...")
    ridge_factor = calculate_ridge_regression_factor(df, lambda_reg=0.1, window=60)

    # 3. è®¡ç®—XGBoostå› å­
    print("\nğŸ“Š æ­¥éª¤ 3/4: è®¡ç®—XGBoostæ³¢åŠ¨ç‡åˆ¶åº¦å› å­...")
    xgb_factor = calculate_xgboost_volatility_factor(df, max_depth=6, learning_rate=0.1, n_estimators=100)

    # 4. åˆå¹¶å› å­
    print("\nğŸ“Š æ­¥éª¤ 4/4: åˆå¹¶å› å­å¹¶é€‰è‚¡...")
    combined = combine_and_standardize_factors(ridge_factor, xgb_factor)

    # ä¿å­˜ç»“æœ
    combined.to_csv('complete_factors_fixed.csv', index=False)
    print(f"\n  âœ“ å› å­æ•°æ®å·²ä¿å­˜: complete_factors_fixed.csv")

    # é€‰è‚¡
    latest_date = combined['datetime'].max()
    stocks, details = select_stocks(combined, latest_date, top_n=50, min_zscore=0.5)

    if details is not None:
        details.to_csv(f'stock_selection_{latest_date.strftime("%Y%m%d")}.csv', index=False)

    print("\n" + "="*70)
    print("âœ… é€‰è‚¡å®Œæˆï¼")
    print("="*70)

    return combined, stocks


if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
