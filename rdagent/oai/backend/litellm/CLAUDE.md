[æ ¹ç›®å½•](../../../../../CLAUDE.md) > [rdagent](../../../../../) > [oai](../../../) > [backend](../) > **litellm**

# LiteLLM åç«¯é›†æˆ

## ç›¸å¯¹è·¯å¾„é¢åŒ…å±‘
[æ ¹ç›®å½•](../../../../../CLAUDE.md) > [rdagent](../../../../../) > [oai](../../../) > [backend](../) > **litellm**

## æ¨¡å—èŒè´£

LiteLLMåç«¯æ˜¯RD-Agentçš„æ ¸å¿ƒLLMé›†æˆæ¨¡å—ï¼Œæä¾›ç»Ÿä¸€çš„LLMè®¿é—®æ¥å£ï¼Œæ”¯æŒå¤šç§LLM Providerçš„ç»Ÿä¸€è°ƒç”¨ã€æˆæœ¬è¿½è¸ªã€é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶ã€‚è¯¥æ¨¡å—åŸºäºLiteLLMåº“æ„å»ºï¼Œä¸ºRD-Agentçš„æ™ºèƒ½ä½“åŠŸèƒ½æä¾›å¼ºå¤§çš„è¯­è¨€æ¨¡å‹æ”¯æŒã€‚

## æ ¸å¿ƒæ¶æ„

### ğŸ”§ litellm.py - æ ¸å¿ƒå®ç°
**åŠŸèƒ½**ï¼šåŸºäºLiteLLMçš„ç»Ÿä¸€LLMåç«¯æ¥å£å®ç°

#### å…³é”®ç‰¹æ€§

**ç»Ÿä¸€æ¥å£æ”¯æŒ**ï¼š
- OpenAI GPTç³»åˆ—ï¼ˆGPT-3.5, GPT-4, GPT-4-turboç­‰ï¼‰
- Azure OpenAIæœåŠ¡
- Anthropic Claudeç³»åˆ—
- æœ¬åœ°æ¨¡å‹ï¼ˆé€šè¿‡OpenAIå…¼å®¹APIï¼‰
- å…¶ä»–æ”¯æŒLiteLLMçš„Provider

**æˆæœ¬ç®¡ç†**ï¼š
- è‡ªåŠ¨Tokenè®¡æ•°å’Œæˆæœ¬è®¡ç®—
- ç´¯è®¡æˆæœ¬è¿½è¸ª
- å¤šProvideræˆæœ¬å¯¹æ¯”
- é¢„ç®—æ§åˆ¶å’Œå‘Šè­¦

**é”™è¯¯å¤„ç†å’Œé‡è¯•**ï¼š
- ç»Ÿä¸€çš„å¼‚å¸¸å¤„ç†æœºåˆ¶
- æ™ºèƒ½é‡è¯•ç­–ç•¥
- è¶…æ—¶é”™è¯¯çš„ç‰¹æ®Šå¤„ç†
- Provideråˆ‡æ¢æ”¯æŒ

**é«˜çº§åŠŸèƒ½æ”¯æŒ**ï¼š
- å‡½æ•°è°ƒç”¨ï¼ˆFunction Callingï¼‰
- æµå¼å“åº”ï¼ˆStreamingï¼‰
- å“åº”æ¨¡å¼ï¼ˆResponse Modeï¼‰
- å¤šæ¨¡æ€è¾“å…¥æ”¯æŒ

### ğŸ› ï¸ é”™è¯¯å¤„ç†ä¼˜åŒ–

#### è¶…æ—¶é”™è¯¯åºåˆ—åŒ–ä¿®å¤
è§£å†³äº†LiteLLMè¶…æ—¶é”™è¯¯æ— æ³•åºåˆ—åŒ–çš„é—®é¢˜ï¼š

```python
import copyreg

def _reduce_no_init(exc):
    """è§£å†³LiteLLMè¶…æ—¶é”™è¯¯çš„åºåˆ—åŒ–é—®é¢˜"""
    cls = exc.__class__
    return (cls.__new__, (cls,), exc.__dict__)

# ä¸ºç‰¹å®šå¼‚å¸¸ç±»å‹æ³¨å†Œåºåˆ—åŒ–æ–¹æ³•
for cls in [BadRequestError, Timeout]:
    copyreg.pickle(cls, _reduce_no_init)
```

**è§£å†³çš„é—®é¢˜**ï¼š
- å¤šè¿›ç¨‹ç¯å¢ƒä¸­è¶…æ—¶é”™è¯¯çš„åºåˆ—åŒ–å¤±è´¥
- åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„é”™è¯¯ä¼ é€’
- ç¼“å­˜å’ŒæŒä¹…åŒ–ä¸­çš„é”™è¯¯å­˜å‚¨

## é…ç½®ç³»ç»Ÿ

### LiteLLMSettings
åŸºäºPydanticçš„é…ç½®ç®¡ç†ç³»ç»Ÿï¼Œæ”¯æŒç¯å¢ƒå˜é‡é…ç½®ï¼š

```python
class LiteLLMSettings(LLMSettings):
    class Config:
        env_prefix = "LITELLM_"
        """ä½¿ç”¨LITELLM_ä½œä¸ºç¯å¢ƒå˜é‡å‰ç¼€"""
```

### ç¯å¢ƒå˜é‡é…ç½®
```bash
# åŸºç¡€é…ç½®
LITELLM_MODEL="gpt-3.5-turbo"
LITELLM_API_BASE="https://api.openai.com/v1"
LITELLM_API_KEY="your_api_key_here"

# Azure OpenAIé…ç½®
LITELLM_API_BASE="https://your-resource.openai.azure.com/"
LITELLM_API_KEY="your_azure_key"
LITELLM_API_VERSION="2023-12-01-preview"

# è¶…æ—¶é…ç½®
LITELLM_TIMEOUT=60
LITELLM_MAX_RETRIES=3

# æˆæœ¬è¿½è¸ªé…ç½®
LITELLM_ENABLE_COST_TRACKING=true
LITELLM_COST_CACHE_TTL=3600
```

## ä½¿ç”¨æ¥å£

### åŸºç¡€ä½¿ç”¨
```python
from rdagent.oai.backend.litellm import LiteLLMAPIBackend

# åˆå§‹åŒ–åç«¯
backend = LiteLLMAPIBackend()

# ç®€å•è°ƒç”¨
response = backend.call(
    prompt="Hello, how are you?",
    model="gpt-3.5-turbo"
)
```

### é«˜çº§åŠŸèƒ½ä½¿ç”¨
```python
# å‡½æ•°è°ƒç”¨
response = backend.call(
    prompt="What's the weather like?",
    model="gpt-3.5-turbo",
    functions=[{
        "name": "get_weather",
        "description": "Get current weather",
        "parameters": {...}
    }]
)

# æµå¼å“åº”
for chunk in backend.stream(
    prompt="Write a story",
    model="gpt-3.5-turbo"
):
    print(chunk, end="")

# å¤šæ¨¡æ€è¾“å…¥
response = backend.call(
    prompt="Describe this image",
    model="gpt-4-vision-preview",
    image_url="https://example.com/image.jpg"
)
```

### æ‰¹é‡è°ƒç”¨
```python
# å¹¶è¡Œè°ƒç”¨
prompts = ["Question 1", "Question 2", "Question 3"]
responses = backend.batch_call(
    prompts=prompts,
    model="gpt-3.5-turbo",
    max_workers=3
)
```

## æ€§èƒ½ä¼˜åŒ–

### ç¼“å­˜æœºåˆ¶
- **å“åº”ç¼“å­˜**ï¼šç›¸åŒè¯·æ±‚çš„æ™ºèƒ½ç¼“å­˜
- **Tokenç¼“å­˜**ï¼šTokenè®¡æ•°ç»“æœç¼“å­˜
- **æˆæœ¬ç¼“å­˜**ï¼šæˆæœ¬è®¡ç®—ç»“æœç¼“å­˜

### å¹¶å‘æ§åˆ¶
- **è¯·æ±‚æ± ç®¡ç†**ï¼šæ§åˆ¶å¹¶å‘è¯·æ±‚æ•°é‡
- **é€Ÿç‡é™åˆ¶**ï¼šéµå®ˆProvideré€Ÿç‡é™åˆ¶
- **è´Ÿè½½å‡è¡¡**ï¼šå¤šProviderè´Ÿè½½åˆ†é…

### èµ„æºç®¡ç†
- **è¿æ¥æ± **ï¼šHTTPè¿æ¥å¤ç”¨
- **å†…å­˜ç®¡ç†**ï¼šå¤§å“åº”çš„æµå¼å¤„ç†
- **è¶…æ—¶æ§åˆ¶**ï¼šç»†ç²’åº¦è¶…æ—¶è®¾ç½®

## ç›‘æ§å’Œè¯Šæ–­

### æˆæœ¬è¿½è¸ª
```python
# è·å–ç´¯è®¡æˆæœ¬
total_cost = backend.get_total_cost()

# æŒ‰Providerç»Ÿè®¡
cost_by_provider = backend.get_cost_by_provider()

# æŒ‰æ¨¡å‹ç»Ÿè®¡
cost_by_model = backend.get_cost_by_model()
```

### æ€§èƒ½ç›‘æ§
```python
# è·å–å“åº”æ—¶é—´ç»Ÿè®¡
response_times = backend.get_response_time_stats()

# è·å–é”™è¯¯ç‡
error_rate = backend.get_error_rate()

# è·å–ä½¿ç”¨é‡ç»Ÿè®¡
usage_stats = backend.get_usage_stats()
```

### è°ƒè¯•å·¥å…·
```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
backend.enable_debug_logging()

# è·å–æœ€è¿‘çš„è¯·æ±‚å†å²
recent_requests = backend.get_recent_requests(limit=10)

# å¯¼å‡ºä½¿ç”¨æŠ¥å‘Š
backend.export_usage_report("usage_report.json")
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**1. APIå¯†é’¥é…ç½®é”™è¯¯**
```bash
# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo $LITELLM_API_KEY

# æµ‹è¯•è¿æ¥
python -c "from rdagent.oai.backend.litellm import LiteLLMAPIBackend; LiteLLMAPIBackend().test_connection()"
```

**2. è¶…æ—¶é”™è¯¯**
```python
# å¢åŠ è¶…æ—¶æ—¶é—´
backend = LiteLLMAPIBackend(timeout=120)

# å¯ç”¨é‡è¯•
backend = LiteLLMAPIBackend(max_retries=5)
```

**3. é€Ÿç‡é™åˆ¶**
```python
# é…ç½®é€Ÿç‡é™åˆ¶
backend = LiteLLMAPIBackend(
    requests_per_minute=60,
    tokens_per_minute=90000
)
```

**4. åºåˆ—åŒ–é”™è¯¯**
- å·²é€šè¿‡copyregä¿®å¤è¶…æ—¶é”™è¯¯çš„åºåˆ—åŒ–é—®é¢˜
- ç¡®ä¿ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ä»£ç 

### è°ƒè¯•æ¨¡å¼
```python
# å¯ç”¨è°ƒè¯•æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯
try:
    response = backend.call(prompt="test", model="gpt-3.5-turbo")
except Exception as e:
    backend.log_error(e)
    raise
```

## æ‰©å±•å¼€å‘

### è‡ªå®šä¹‰Provider
```python
class CustomProvider(LiteLLMAPIBackend):
    def custom_call(self, prompt, **kwargs):
        # è‡ªå®šä¹‰è°ƒç”¨é€»è¾‘
        pass

    def custom_auth(self):
        # è‡ªå®šä¹‰è®¤è¯é€»è¾‘
        pass
```

### è‡ªå®šä¹‰ä¸­é—´ä»¶
```python
class LoggingMiddleware:
    def before_call(self, request):
        print(f"Calling: {request}")

    def after_call(self, response):
        print(f"Response: {response}")

backend.add_middleware(LoggingMiddleware())
```

### é’©å­å‡½æ•°
```python
# æ³¨å†Œè°ƒç”¨å‰é’©å­
def before_call_hook(prompt, model):
    # é¢„å¤„ç†é€»è¾‘
    return processed_prompt, model

backend.register_before_call_hook(before_call_hook)

# æ³¨å†Œè°ƒç”¨åé’©å­
def after_call_hook(response):
    # åå¤„ç†é€»è¾‘
    return processed_response

backend.register_after_call_hook(after_call_hook)
```

## æœ€ä½³å®è·µ

### 1. é…ç½®ç®¡ç†
- ä½¿ç”¨ç¯å¢ƒå˜é‡ç®¡ç†æ•æ„Ÿä¿¡æ¯
- ä¸ºä¸åŒç¯å¢ƒåˆ›å»ºä¸åŒçš„é…ç½®æ–‡ä»¶
- å®šæœŸè½®æ¢APIå¯†é’¥

### 2. æˆæœ¬æ§åˆ¶
- è®¾ç½®æœˆåº¦é¢„ç®—é™åˆ¶
- ç›‘æ§å„é¡¹ç›®/å›¢é˜Ÿçš„ç”¨é‡
- é€‰æ‹©æ€§ä»·æ¯”æœ€é«˜çš„æ¨¡å‹

### 3. æ€§èƒ½ä¼˜åŒ–
- åˆç†ä½¿ç”¨ç¼“å­˜å‡å°‘é‡å¤è¯·æ±‚
- æ‰¹é‡å¤„ç†æé«˜æ•ˆç‡
- é€‰æ‹©åˆé€‚çš„æ¨¡å‹å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡

### 4. é”™è¯¯å¤„ç†
- å®ç°ä¼˜é›…çš„é™çº§ç­–ç•¥
- ç›‘æ§é”™è¯¯ç‡å¹¶è®¾ç½®å‘Šè­¦
- å‡†å¤‡å¤‡ç”¨Provider

---

## å˜æ›´è®°å½• (Changelog)

### 2025-12-06 - è¶…æ—¶é”™è¯¯ä¿®å¤å’Œç¨³å®šæ€§å¢å¼º
- **åºåˆ—åŒ–é—®é¢˜ä¿®å¤**ï¼šè§£å†³LiteLLMè¶…æ—¶é”™è¯¯åœ¨å¤šè¿›ç¨‹ç¯å¢ƒä¸­çš„åºåˆ—åŒ–é—®é¢˜
- **é”™è¯¯å¤„ç†ä¼˜åŒ–**ï¼šå¢å¼ºå¼‚å¸¸å¤„ç†æœºåˆ¶ï¼Œæé«˜ç³»ç»Ÿç¨³å®šæ€§
- **æˆæœ¬è¿½è¸ªæ”¹è¿›**ï¼šä¼˜åŒ–æˆæœ¬è®¡ç®—å’Œç¼“å­˜æœºåˆ¶
- **æ€§èƒ½ç›‘æ§å¢å¼º**ï¼šæ–°å¢è¯¦ç»†çš„æ€§èƒ½ç›‘æ§å’Œè¯Šæ–­å·¥å…·
- **é…ç½®ç³»ç»Ÿå®Œå–„**ï¼šåŸºäºPydanticçš„é…ç½®ç®¡ç†ç³»ç»Ÿ
- **æ–‡æ¡£ä½“ç³»å»ºç«‹**ï¼šå®Œæ•´çš„æ¨¡å—æ–‡æ¡£å’Œä½¿ç”¨æŒ‡å—

### 2025-11-17 - æ¨¡å—åˆå§‹åŒ–
- **åŸºç¡€æ¶æ„å»ºç«‹**ï¼šLiteLLMåç«¯æ ¸å¿ƒæ¡†æ¶æ­å»º
- **å¤šProvideræ”¯æŒ**ï¼šé›†æˆä¸»æµLLM Provider
- **ç»Ÿä¸€æ¥å£è®¾è®¡**ï¼šä¸€è‡´çš„APIè°ƒç”¨æ¥å£
- **æˆæœ¬ç®¡ç†ç³»ç»Ÿ**ï¼šè‡ªåŠ¨æˆæœ¬è¿½è¸ªå’Œç»Ÿè®¡

---

*æœ€åæ›´æ–°ï¼š2025-12-06*