#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è¡Œä¸šç›¸å¯¹å¼ºåº¦å› å­ç¤ºä¾‹
==================

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨è¡Œä¸šåˆ†ç±»æ•°æ®åˆ›å»ºè¡Œä¸šä¸­æ€§å› å­ã€‚

è¡Œä¸šä¸­æ€§å› å­çš„ä¼˜åŠ¿ï¼š
- æ¶ˆé™¤è¡Œä¸šåå·®ï¼Œé¿å…è¡Œä¸šé›†ä¸­é£é™©
- åœ¨è¡Œä¸šå†…é€‰è‚¡ï¼Œæ›´å…¬å¹³åœ°æ¯”è¾ƒå…¬å¸
- é™ä½ç»„åˆçš„å›æ’¤å’Œæ³¢åŠ¨ç‡

æ•°æ®æ¥æºï¼š
- ä»·æ ¼/è´¢åŠ¡æ•°æ®ï¼šgit_ignore_folder/factor_implementation_source_data/daily_pv.h5
- è¡Œä¸šåˆ†ç±»ï¼š~/.qlib/qlib_data/cn_data/industry_data/tushare_stock_to_industry_dict_*.json

ç”³ä¸‡ 2021 L2 è¡Œä¸šåˆ†ç±»ï¼š
- 110 ä¸ªäºŒçº§è¡Œä¸š
- è¦†ç›– 5,466 åª A è‚¡
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path
from typing import Dict


def load_data(data_path: str = None) -> pd.DataFrame:
    """åŠ è½½ daily_pv.h5 æ•°æ®"""
    if data_path is None:
        data_path = Path.home() / "git_ignore_folder/factor_implementation_source_data/daily_pv.h5"

    print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®: {data_path}")
    df = pd.read_hdf(data_path, key="data")
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼å½¢çŠ¶: {df.shape}")
    return df


def load_industry_mapping() -> Dict[str, Dict]:
    """
    åŠ è½½ç”³ä¸‡ 2021 L2 è¡Œä¸šåˆ†ç±»æ˜ å°„

    Returns:
        è¡Œä¸šæ˜ å°„å­—å…¸: {è‚¡ç¥¨ä»£ç : {'industry_l1': ä¸€çº§è¡Œä¸š, 'industry_l2': äºŒçº§è¡Œä¸š}}
    """
    print("\nğŸ“‚ æ­£åœ¨åŠ è½½è¡Œä¸šåˆ†ç±»æ˜ å°„...")

    # æŸ¥æ‰¾æœ€æ–°çš„è¡Œä¸šåˆ†ç±»æ–‡ä»¶
    industry_dir = Path.home() / ".qlib/qlib_data/cn_data/industry_data"

    # å°è¯•æŸ¥æ‰¾ tushare è¡Œä¸šåˆ†ç±»æ–‡ä»¶
    industry_files = list(industry_dir.glob("tushare_stock_to_industry_dict_*.json"))

    if not industry_files:
        raise FileNotFoundError(
            f"âŒ åœ¨ {industry_dir} ä¸­æ‰¾ä¸åˆ°è¡Œä¸šåˆ†ç±»æ–‡ä»¶ï¼\n"
            f"   è¯·å…ˆè¿è¡Œæ•°æ®å‡†å¤‡è„šæœ¬ç”Ÿæˆè¡Œä¸šåˆ†ç±»ã€‚"
        )

    # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
    industry_file = sorted(industry_files)[-1]
    print(f"   ä½¿ç”¨æ–‡ä»¶: {industry_file.name}")

    with open(industry_file, "r", encoding="utf-8") as f:
        industry_mapping = json.load(f)

    print(f"âœ… è¡Œä¸šåˆ†ç±»åŠ è½½å®Œæˆï¼")
    print(f"   - è¦†ç›–è‚¡ç¥¨æ•°: {len(industry_mapping):,}")

    # ç»Ÿè®¡è¡Œä¸šæ•°é‡
    l1_industries = set(v.get("industry_l1", "") for v in industry_mapping.values())
    l2_industries = set(v.get("industry_l2", "") for v in industry_mapping.values())

    print(f"   - ä¸€çº§è¡Œä¸šæ•°: {len(l1_industries)}")
    print(f"   - äºŒçº§è¡Œä¸šæ•°: {len(l2_industries)}")

    return industry_mapping


def map_industry_to_dataframe(df: pd.DataFrame, industry_mapping: Dict) -> pd.DataFrame:
    """
    å°†è¡Œä¸šåˆ†ç±»æ˜ å°„åˆ° DataFrame

    Args:
        df: è¾“å…¥æ•°æ®
        industry_mapping: è¡Œä¸šæ˜ å°„å­—å…¸

    Returns:
        æ·»åŠ äº†è¡Œä¸šåˆ—çš„ DataFrame
    """
    print("\nğŸ” å°†è¡Œä¸šåˆ†ç±»æ˜ å°„åˆ°æ•°æ®...")

    df_reset = df.reset_index()

    # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç æ ¼å¼ï¼ˆç§»é™¤äº¤æ˜“æ‰€åç¼€ï¼‰
    df_reset["stock_code"] = df_reset["instrument"].str.replace(".", "")

    # æ˜ å°„ä¸€çº§è¡Œä¸š
    df_reset["industry_l1"] = df_reset["stock_code"].map(
        lambda x: industry_mapping.get(x, {}).get("industry_l1", "Unknown")
    )

    # æ˜ å°„äºŒçº§è¡Œä¸š
    df_reset["industry_l2"] = df_reset["stock_code"].map(
        lambda x: industry_mapping.get(x, {}).get("industry_l2", "Unknown")
    )

    # ç»Ÿè®¡æ˜ å°„æˆåŠŸç‡
    total_stocks = len(df_reset["instrument"].unique())
    mapped_l1 = df_reset[df_reset["industry_l1"] != "Unknown"]["instrument"].nunique()
    mapped_l2 = df_reset[df_reset["industry_l2"] != "Unknown"]["instrument"].nunique()

    print(f"âœ… è¡Œä¸šæ˜ å°„å®Œæˆï¼")
    print(f"   - æ€»è‚¡ç¥¨æ•°: {total_stocks:,}")
    print(f"   - ä¸€çº§è¡Œä¸šæ˜ å°„æˆåŠŸ: {mapped_l1:,} ({mapped_l1/total_stocks*100:.1f}%)")
    print(f"   - äºŒçº§è¡Œä¸šæ˜ å°„æˆåŠŸ: {mapped_l2:,} ({mapped_l2/total_stocks*100:.1f}%)")

    # æ˜¾ç¤ºè¡Œä¸šåˆ†å¸ƒ
    print(f"\nğŸ“Š äºŒçº§è¡Œä¸šåˆ†å¸ƒï¼ˆTop 10ï¼‰:")
    l2_dist = df_reset[df_reset["industry_l2"] != "Unknown"]["industry_l2"].value_counts().head(10)
    for industry, count in l2_dist.items():
        print(f"   {industry}: {count:,} æ¡æ•°æ®")

    return df_reset


def calculate_industry_relative_pe(df: pd.DataFrame) -> pd.DataFrame:
    """
    è¡Œä¸šç›¸å¯¹ PE å› å­

    é€»è¾‘ï¼š
    - åœ¨æ¯ä¸ªè¡Œä¸šå†…è®¡ç®— PE çš„ç›¸å¯¹æ’å
    - ä½ PEï¼ˆç›¸å¯¹è¡Œä¸šï¼‰= é«˜ä»·å€¼
    - è¿™æ ·å¯ä»¥æ¶ˆé™¤è¡Œä¸šé—´çš„ä¼°å€¼å·®å¼‚

    å­¦æœ¯ä¾æ®ï¼š
    - è¡Œä¸šä¸­æ€§ç­–ç•¥æ˜¯é‡åŒ–æŠ•èµ„çš„æ ‡å‡†åšæ³•
    - é¿å…è¡Œä¸šé›†ä¸­ï¼Œé™ä½ç»„åˆé£é™©
    - åœ¨è¡Œä¸šå†…æ¯”è¾ƒæ›´å…¬å¹³

    Args:
        df: åŒ…å«è¡Œä¸šä¿¡æ¯çš„ DataFrame

    Returns:
        åŒ…å«è¡Œä¸šç›¸å¯¹ PE å› å­çš„ DataFrame
    """
    print("\n" + "=" * 70)
    print("ğŸ“Š è¡Œä¸šç›¸å¯¹ PE å› å­ (Industry-Relative PE)")
    print("=" * 70)

    # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
    df_valid = df[(df["PE"].notna()) & (df["industry_l2"] != "Unknown")].copy()

    print(f"\nğŸ” Step 1: è®¡ç®—è¡Œä¸šå†… PE æ’å")
    print("-" * 70)

    # åœ¨æ¯ä¸ªè¡Œä¸šå†…è®¡ç®— PE ç™¾åˆ†ä½æ’å
    df_valid["PE_rank_industry"] = df_valid.groupby(["datetime", "industry_l2"])["PE"].transform(
        lambda x: x.rank(pct=True)
    )

    # å–å€’æ•°ï¼šä½ PE = é«˜ä»·å€¼
    df_valid["Industry_Relative_PE"] = 1 - df_valid["PE_rank_industry"]

    print(f"âœ… è¡Œä¸šç›¸å¯¹ PE è®¡ç®—å®Œæˆ")
    print(f"   - æœ‰æ•ˆæ•°æ®ç‚¹: {len(df_valid):,}")
    print(f"   - æ¶‰åŠè¡Œä¸šæ•°: {df_valid['industry_l2'].nunique()}")
    print(f"   - å› å­å‡å€¼: {df_valid['Industry_Relative_PE'].mean():.4f}")
    print(f"   - å› å­æ ‡å‡†å·®: {df_valid['Industry_Relative_PE'].std():.4f}")

    # åˆ†æè¡Œä¸šåˆ†å¸ƒ
    print(f"\nğŸ” Step 2: è¡Œä¸šä»·å€¼åˆ†å¸ƒåˆ†æ")
    print("-" * 70)

    sample_date = df_valid["datetime"].max()
    sample_df = df_valid[df_valid["datetime"] == sample_date]

    # æŒ‰è¡Œä¸šç»Ÿè®¡å¹³å‡ä»·å€¼å¾—åˆ†
    industry_value = sample_df.groupby("industry_l2")["Industry_Relative_PE"].agg(["mean", "count"])
    industry_value = industry_value.sort_values("mean", ascending=False)

    print(f"\næœ€æ–°æ—¶ç‚¹å„è¡Œä¸šå¹³å‡ä»·å€¼å¾—åˆ†ï¼ˆTop 10 ä½ä¼°å€¼è¡Œä¸šï¼‰:")
    for idx, (industry, row) in enumerate(industry_value.head(10).iterrows(), 1):
        print(f"   {idx:2d}. {industry:30s}: {row['mean']:.4f} (n={row['count']})")

    print(f"\næœ€æ–°æ—¶ç‚¹å„è¡Œä¸šå¹³å‡ä»·å€¼å¾—åˆ†ï¼ˆBottom 5 é«˜ä¼°å€¼è¡Œä¸šï¼‰:")
    for idx, (industry, row) in enumerate(industry_value.tail(5).iterrows(), 1):
        print(f"   {idx}. {industry:30s}: {row['mean']:.4f} (n={row['count']})")

    # å±•ç¤ºç¤ºä¾‹è‚¡ç¥¨
    print(f"\nğŸ” Step 3: ç¤ºä¾‹è‚¡ç¥¨åˆ†æï¼ˆæœ€æ–°æ—¶ç‚¹ï¼‰")
    print("-" * 70)

    # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§è¡Œä¸š
    for industry in ["è¯åˆ¸", "é“¶è¡Œ", "ç™½é…’", "åŠå¯¼ä½“"]:
        industry_stocks = sample_df[sample_df["industry_l2"] == industry]
        if len(industry_stocks) > 0:
            print(f"\n   ã€{industry}ã€‘è¡Œä¸šå†…ä»·å€¼æ’å Top 3:")

            top3 = industry_stocks.nlargest(3, "Industry_Relative_PE")
            for _, row in top3.iterrows():
                print(f"      {row['instrument']:12s}: "
                      f"PE={row['PE']:7.2f}, "
                      f"è¡Œä¸šç›¸å¯¹PE={row['Industry_Relative_PE']:.4f}")

    # è¿”å› MultiIndex æ ¼å¼
    result = df_valid.set_index(["datetime", "instrument"])
    return result[["Industry_Relative_PE"]]


def calculate_industry_relative_roe(df: pd.DataFrame) -> pd.DataFrame:
    """
    è¡Œä¸šç›¸å¯¹ ROE å› å­

    é€»è¾‘ï¼š
    - åœ¨æ¯ä¸ªè¡Œä¸šå†…è®¡ç®— ROE çš„ç›¸å¯¹å¼ºåº¦
    - é«˜ ROEï¼ˆç›¸å¯¹è¡Œä¸šï¼‰= é«˜ç›ˆåˆ©èƒ½åŠ›
    - è¿™æ ·å¯ä»¥è¯†åˆ«è¡Œä¸šå†…çš„ä¼˜è´¨å…¬å¸

    Args:
        df: åŒ…å«è¡Œä¸šä¿¡æ¯çš„ DataFrame

    Returns:
        åŒ…å«è¡Œä¸šç›¸å¯¹ ROE å› å­çš„ DataFrame
    """
    print("\n" + "=" * 70)
    print("ğŸ“Š è¡Œä¸šç›¸å¯¹ ROE å› å­ (Industry-Relative ROE)")
    print("=" * 70)

    # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
    df_valid = df[(df["ROE"].notna()) & (df["industry_l2"] != "Unknown")].copy()

    print(f"\nğŸ” Step 1: è®¡ç®—è¡Œä¸šå†… ROE ç›¸å¯¹å¼ºåº¦")
    print("-" * 70)

    # åœ¨æ¯ä¸ªè¡Œä¸šå†…è®¡ç®— ROE z-score
    df_valid["ROE_zscore_industry"] = df_valid.groupby(["datetime", "industry_l2"])["ROE"].transform(
        lambda x: (x - x.mean()) / (x.std() + 1e-12)
    )

    print(f"âœ… è¡Œä¸šç›¸å¯¹ ROE è®¡ç®—å®Œæˆ")
    print(f"   - æœ‰æ•ˆæ•°æ®ç‚¹: {len(df_valid):,}")
    print(f"   - æ¶‰åŠè¡Œä¸šæ•°: {df_valid['industry_l2'].nunique()}")
    print(f"   - å› å­å‡å€¼: {df_valid['ROE_zscore_industry'].mean():.4f}")
    print(f"   - å› å­æ ‡å‡†å·®: {df_valid['ROE_zscore_industry'].std():.4f}")

    # åˆ†æè¡Œä¸šç›ˆåˆ©èƒ½åŠ›åˆ†å¸ƒ
    print(f"\nğŸ” Step 2: è¡Œä¸šç›ˆåˆ©èƒ½åŠ›åˆ†å¸ƒåˆ†æ")
    print("-" * 70)

    sample_date = df_valid["datetime"].max()
    sample_df = df_valid[df_valid["datetime"] == sample_date]

    # æŒ‰è¡Œä¸šç»Ÿè®¡å¹³å‡ ROE ç›¸å¯¹å¼ºåº¦
    industry_roe = sample_df.groupby("industry_l2")["ROE_zscore_industry"].agg(["mean", "count"])
    industry_roe = industry_roe.sort_values("mean", ascending=False)

    print(f"\næœ€æ–°æ—¶ç‚¹å„è¡Œä¸šå¹³å‡ç›ˆåˆ©èƒ½åŠ›ï¼ˆTop 10 å¼ºåŠ¿è¡Œä¸šï¼‰:")
    for idx, (industry, row) in enumerate(industry_roe.head(10).iterrows(), 1):
        print(f"   {idx:2d}. {industry:30s}: {row['mean']:.4f} (n={row['count']})")

    # å±•ç¤ºç¤ºä¾‹è‚¡ç¥¨
    print(f"\nğŸ” Step 3: ç¤ºä¾‹è‚¡ç¥¨åˆ†æï¼ˆæœ€æ–°æ—¶ç‚¹ï¼‰")
    print("-" * 70)

    # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§è¡Œä¸š
    for industry in ["ç™½é…’", "é“¶è¡Œ", "è¯åˆ¸", "åŠå¯¼ä½“"]:
        industry_stocks = sample_df[sample_df["industry_l2"] == industry]
        if len(industry_stocks) > 0:
            print(f"\n   ã€{industry}ã€‘è¡Œä¸šå†… ROE ç›¸å¯¹å¼ºåº¦ Top 3:")

            top3 = industry_stocks.nlargest(3, "ROE_zscore_industry")
            for _, row in top3.iterrows():
                print(f"      {row['instrument']:12s}: "
                      f"ROE={row['ROE']:6.2f}%, "
                      f"è¡Œä¸šç›¸å¯¹ROE={row['ROE_zscore_industry']:.4f}")

    # è¿”å› MultiIndex æ ¼å¼
    result = df_valid.set_index(["datetime", "instrument"])
    return result[["ROE_zscore_industry"]]


def calculate_industry_neutral_momentum(df: pd.DataFrame) -> pd.DataFrame:
    """
    è¡Œä¸šä¸­æ€§åŠ¨é‡å› å­

    é€»è¾‘ï¼š
    - å…ˆè®¡ç®—åŸå§‹åŠ¨é‡ï¼ˆ20æ—¥æ”¶ç›Šç‡ï¼‰
    - å†åœ¨æ¯ä¸ªè¡Œä¸šå†…æ ‡å‡†åŒ–
    - è¿™æ ·å¯ä»¥æ¶ˆé™¤è¡Œä¸šæ•´ä½“åŠ¨é‡çš„å½±å“

    ä¼˜åŠ¿ï¼š
    - é¿å…è¿½é«˜çƒ­é—¨è¡Œä¸š
    - åœ¨è¡Œä¸šå†…é€‰æ‹©å¼ºåŠ¿è‚¡ç¥¨
    - é™ä½è¡Œä¸šè½®åŠ¨é£é™©

    Args:
        df: åŒ…å«è¡Œä¸šä¿¡æ¯çš„ DataFrame

    Returns:
        åŒ…å«è¡Œä¸šä¸­æ€§åŠ¨é‡å› å­çš„ DataFrame
    """
    print("\n" + "=" * 70)
    print("ğŸ“Š è¡Œä¸šä¸­æ€§åŠ¨é‡å› å­ (Industry-Neutral Momentum)")
    print("=" * 70)

    # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
    df_valid = df[(df["industry_l2"] != "Unknown")].copy()

    print(f"\nğŸ” Step 1: è®¡ç®— 20 æ—¥æ”¶ç›Šç‡")
    print("-" * 70)

    # è®¡ç®—æ”¶ç›Šç‡
    df_valid["returns_20d"] = df_valid.groupby("instrument")["$close"].transform(
        lambda x: x.pct_change(periods=20)
    )

    print(f"âœ… æ”¶ç›Šç‡è®¡ç®—å®Œæˆ")

    print(f"\nğŸ” Step 2: è¡Œä¸šå†…æ”¶ç›Šç‡æ ‡å‡†åŒ–")
    print("-" * 70)

    # åœ¨æ¯ä¸ªè¡Œä¸šå†…æ ‡å‡†åŒ–æ”¶ç›Šç‡
    df_valid["Industry_Neutral_Momentum"] = df_valid.groupby(["datetime", "industry_l2"])["returns_20d"].transform(
        lambda x: (x - x.mean()) / (x.std() + 1e-12)
    )

    # ç§»é™¤ NaN
    df_valid = df_valid[df_valid["Industry_Neutral_Momentum"].notna()]

    print(f"âœ… è¡Œä¸šä¸­æ€§åŠ¨é‡è®¡ç®—å®Œæˆ")
    print(f"   - æœ‰æ•ˆæ•°æ®ç‚¹: {len(df_valid):,}")
    print(f"   - æ¶‰åŠè¡Œä¸šæ•°: {df_valid['industry_l2'].nunique()}")

    # åˆ†æè¡Œä¸šåŠ¨é‡åˆ†å¸ƒ
    print(f"\nğŸ” Step 3: è¡Œä¸šåŠ¨é‡åˆ†å¸ƒåˆ†æ")
    print("-" * 70)

    sample_date = df_valid["datetime"].max()
    sample_df = df_valid[df_valid["datetime"] == sample_date]

    # æŒ‰è¡Œä¸šç»Ÿè®¡å¹³å‡åŠ¨é‡
    industry_momentum = sample_df.groupby("industry_l2")["Industry_Neutral_Momentum"].agg(["mean", "std", "count"])
    industry_momentum = industry_momentum.sort_values("mean", ascending=False)

    print(f"\næœ€æ–°æ—¶ç‚¹å„è¡Œä¸šå¹³å‡åŠ¨é‡ï¼ˆTop 10 å¼ºåŠ¿è¡Œä¸šï¼‰:")
    for idx, (industry, row) in enumerate(industry_momentum.head(10).iterrows(), 1):
        print(f"   {idx:2d}. {industry:30s}: {row['mean']:.4f} (std={row['std']:.4f}, n={row['count']})")

    # å±•ç¤ºç¤ºä¾‹è‚¡ç¥¨
    print(f"\nğŸ” Step 4: ç¤ºä¾‹è‚¡ç¥¨åˆ†æï¼ˆæœ€æ–°æ—¶ç‚¹ï¼‰")
    print("-" * 70)

    # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§è¡Œä¸š
    for industry in ["ç™½é…’", "æ–°èƒ½æºå‘ç”µ", "é“¶è¡Œ"]:
        industry_stocks = sample_df[sample_df["industry_l2"] == industry]
        if len(industry_stocks) > 0:
            print(f"\n   ã€{industry}ã€‘è¡Œä¸šä¸­æ€§åŠ¨é‡ Top 3:")

            top3 = industry_stocks.nlargest(3, "Industry_Neutral_Momentum")
            for _, row in top3.iterrows():
                print(f"      {row['instrument']:12s}: "
                      f"æ”¶ç›Šç‡={row['returns_20d']:6.2%}, "
                      f"è¡Œä¸šä¸­æ€§åŠ¨é‡={row['Industry_Neutral_Momentum']:.4f}")

    # è¿”å› MultiIndex æ ¼å¼
    result = df_valid.set_index(["datetime", "instrument"])
    return result[["Industry_Neutral_Momentum"]]


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("ğŸ¯ è¡Œä¸šç›¸å¯¹å¼ºåº¦å› å­ç¤ºä¾‹")
    print("=" * 70)
    print("\nğŸ“š ç†è®ºåŸºç¡€:")
    print("   - è¡Œä¸šä¸­æ€§ç­–ç•¥æ˜¯é‡åŒ–æŠ•èµ„çš„æ ‡å‡†åšæ³•")
    print("   - æ¶ˆé™¤è¡Œä¸šåå·®ï¼Œé¿å…è¡Œä¸šé›†ä¸­é£é™©")
    print("   - åœ¨è¡Œä¸šå†…é€‰è‚¡ï¼Œæ›´å…¬å¹³åœ°æ¯”è¾ƒå…¬å¸")
    print("   - é™ä½ç»„åˆå›æ’¤å’Œæ³¢åŠ¨ç‡")

    try:
        # åŠ è½½æ•°æ®
        df = load_data()

        # åŠ è½½è¡Œä¸šåˆ†ç±»
        industry_mapping = load_industry_mapping()

        # æ˜ å°„è¡Œä¸šåˆ°æ•°æ®
        df_with_industry = map_industry_to_dataframe(df, industry_mapping)

        # è®¡ç®—å„ä¸ªè¡Œä¸šä¸­æ€§å› å­
        pe_result = calculate_industry_relative_pe(df_with_industry)
        roe_result = calculate_industry_relative_roe(df_with_industry)
        momentum_result = calculate_industry_neutral_momentum(df_with_industry)

        # åˆå¹¶æ‰€æœ‰å› å­
        print("\n" + "=" * 70)
        print("ğŸ“Š åˆå¹¶æ‰€æœ‰è¡Œä¸šä¸­æ€§å› å­")
        print("=" * 70)

        all_factors = pd.concat([pe_result, roe_result, momentum_result], axis=1)
        print(f"âœ… å› å­åˆå¹¶å®Œæˆï¼å½¢çŠ¶: {all_factors.shape}")

        # å› å­ç›¸å…³æ€§åˆ†æ
        print(f"\nå› å­ç›¸å…³æ€§çŸ©é˜µ:")
        print(all_factors.corr())

        # ä¿å­˜ç»“æœ
        output_path = Path("ex03_industry_relative_strength_output.h5")
        all_factors.to_hdf(output_path, key="data")
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

        print("\n" + "=" * 70)
        print("âœ¨ æ‰€æœ‰è¡Œä¸šç›¸å¯¹å¼ºåº¦å› å­è®¡ç®—å®Œæˆï¼")
        print("=" * 70)
        print("\nğŸ¯ ä½¿ç”¨å»ºè®®:")
        print("   1. è¡Œä¸šä¸­æ€§å› å­é€‚åˆæ„å»ºç¨³å¥çš„å¤šå› å­ç»„åˆ")
        print("   2. å¯ä»¥ä¸å…¨å¸‚åœºå› å­ç»“åˆï¼Œå¹³è¡¡è¡Œä¸šæš´éœ²")
        print("   3. å»ºè®®å®šæœŸæ£€æŸ¥è¡Œä¸šåˆ†å¸ƒï¼Œé¿å…éšå«è¡Œä¸šåå·®")
        print("   4. ä½¿ç”¨ Qlib æ¡†æ¶è¿›è¡Œå®Œæ•´å›æµ‹éªŒè¯")

    except FileNotFoundError as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print("   1. ç¡®ä¿å·²å‡†å¤‡è¡Œä¸šåˆ†ç±»æ•°æ®")
        print("   2. è¿è¡Œæ•°æ®å‡†å¤‡è„šæœ¬ç”Ÿæˆè¡Œä¸šæ˜ å°„æ–‡ä»¶")
        print("   3. æ£€æŸ¥æ•°æ®è·¯å¾„æ˜¯å¦æ­£ç¡®")


if __name__ == "__main__":
    main()
