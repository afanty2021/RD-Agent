"""
åŸºå‡†æµ‹è¯•å·¥å…·æ¨¡å—

æä¾›æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œæ¯”è¾ƒåŠŸèƒ½
"""

import time
import statistics
from typing import Dict, List, Any, Callable
from contextlib import contextmanager


class BenchmarkRunner:
    """åŸºå‡†æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self):
        self.results = []

    def run_benchmark(self, func: Callable, iterations: int = 100) -> Dict[str, Any]:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        times = []

        for i in range(iterations):
            start_time = time.perf_counter()
            result = func()
            end_time = time.perf_counter()
            execution_time = end_time - start_time
            times.append(execution_time)

        return {
            'function': func.__name__,
            'iterations': iterations,
            'results': {
                'min': min(times),
                'max': max(times),
                'avg': statistics.mean(times),
                'median': statistics.median(times),
                'p95': statistics.quantiles(times, 0.95)[0],
                'p99': statistics.quantiles(times, 0.99)[0],
                'std': statistics.stdev(times)
            }
        }

    def compare_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ¯”è¾ƒå¤šä¸ªåŸºå‡†æµ‹è¯•ç»“æœ"""
        if len(results) < 2:
            return {'status': 'insufficient_data', 'message': 'éœ€è¦è‡³å°‘2ä¸ªç»“æœè¿›è¡Œæ¯”è¾ƒ'}

        baseline = results[0]
        improvements = []

        for i, result in enumerate(results[1:], 1):
            improvement = {}

            for metric in ['min', 'max', 'avg', 'median', 'p95', 'p99', 'std']:
                if result['results'][metric] < baseline['results'][metric]:
                    improvement[metric] = {
                        'baseline': baseline['results'][metric],
                        'current': result['results'][metric],
                        'improvement': baseline['results'][metric] - result['results'][metric],
                        'improvement_pct': ((baseline['results'][metric] - result['results'][metric]) / baseline['results'][metric]) * 100
                    }

            if improvement:
                improvements.append(improvement)

        return {
            'status': 'success',
            'baseline': baseline,
            'comparisons': results[1:],
            'improvements': improvements
        }


def run_quick_benchmark():
    """è¿è¡Œå¿«é€ŸåŸºå‡†æµ‹è¯•"""
    runner = BenchmarkRunner()

    # æµ‹è¯•å¿«é€Ÿå‡½æ•°
    fast_results = runner.run_benchmark(fast_function, 50)
    print("ğŸš€ å¿«é€Ÿå‡½æ•°åŸºå‡†æµ‹è¯•:")
    print(f"  å¹³å‡è€—æ—¶: {fast_results['results']['avg']:.4f}s")
    print(f"  95%åˆ†ä½æ•°: {fast_results['results']['p95']:.4f}s")

    # æµ‹è¯•æ…¢é€Ÿå‡½æ•°
    slow_results = runner.run_benchmark(slow_function, 50)
    print("\nğŸŒ æ…¢é€Ÿå‡½æ•°åŸºå‡†æµ‹è¯•:")
    print(f"  å¹³å‡è€—æ—¶: {slow_results['results']['avg']:.4f}s")
    print(f"  95%åˆ†ä½æ•°: {slow_results['results']['p95']:.4f}s")

    # æ¯”è¾ƒç»“æœ
    comparison = runner.compare_results([fast_results, slow_results])
    if comparison['status'] == 'success':
        print("\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
        for improvement in comparison['improvements']:
            metric_name = improvement.get('metric', 'unknown')
            baseline_val = improvement.get('baseline', 0)
            current_val = improvement.get('current', 0)
            improvement_pct = improvement.get('improvement_pct', 0)

            print(f"  {metric_name}: {baseline_val:.4f}s â†’ {current_val:.4f}s")
            if improvement_pct > 0:
                print(f"  æ”¹å–„: {improvement_pct:.1f}%")
            else:
                print(f"  æ¶åŒ–: {improvement_pct:.1f}%")


if __name__ == "__main__":
    run_quick_benchmark()