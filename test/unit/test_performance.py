"""
æ€§èƒ½ç›‘æ§å’ŒåŸºå‡†æµ‹è¯•æ¨¡å—æµ‹è¯•
"""

import pytest
from rdagent.utils.performance import PerformanceMonitor, monitor_performance
from rdagent.utils.benchmark import BenchmarkRunner


def test_performance_monitor_basic():
    """æµ‹è¯•æ€§èƒ½ç›‘æ§åŸºæœ¬åŠŸèƒ½"""
    monitor = PerformanceMonitor()

    @monitor_performance
    def test_function():
        time.sleep(0.1)
        return "test_completed"

    @monitor_performance
    def test_function_with_error():
        raise ValueError("æµ‹è¯•é”™è¯¯")

    # æ‰§è¡Œæµ‹è¯•
    with pytest.raises(ValueError):
        test_function_with_error()


def test_benchmark_runner_basic():
    """æµ‹è¯•åŸºå‡†è¿è¡Œå™¨åŸºæœ¬åŠŸèƒ½"""
    runner = BenchmarkRunner()

    # æµ‹è¯•åŸºå‡†æµ‹è¯•åŠŸèƒ½
    results = runner.run_benchmark(test_function, 10)
    assert results['status'] == 'success'
    assert 'results' in results
    assert 'avg' in results['results']
    assert isinstance(results['results']['avg'], (int, float))


def test_benchmark_runner_comparison():
    """æµ‹è¯•åŸºå‡†æ¯”è¾ƒåŠŸèƒ½"""
    runner = BenchmarkRunner()

    # è¿è¡Œå¿«é€Ÿå’Œæ…¢é€Ÿæµ‹è¯•
    fast_results = runner.run_benchmark(test_function, 50)
    slow_results = runner.run_benchmark(slow_function, 50)

    # æ¯”è¾ƒç»“æœ
    comparison = runner.compare_results([fast_results, slow_results])
    assert comparison['status'] == 'success'
    assert 'improvements' in comparison


if __name__ == "__main__":
    test_performance_monitor_basic()
    test_benchmark_runner_basic()
    test_benchmark_runner_comparison()

    print("\nğŸ§ª æ‰€æœ‰æ€§èƒ½æµ‹è¯•é€šè¿‡!")