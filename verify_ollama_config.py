#!/usr/bin/env python3
"""
éªŒè¯ Ollama BGE-M3 é…ç½®
æ£€æŸ¥ RD-Agent é…ç½®æ–‡ä»¶å’Œç¯å¢ƒå˜é‡
"""

import os
import sys
from pathlib import Path

def check_env_file():
    """æ£€æŸ¥ .env æ–‡ä»¶é…ç½®"""
    print("=" * 60)
    print("ğŸ“‹ .env æ–‡ä»¶é…ç½®æ£€æŸ¥")
    print("=" * 60)

    env_path = Path.cwd() / ".env"
    if not env_path.exists():
        print("âŒ .env æ–‡ä»¶ä¸å­˜åœ¨")
        return False

    print(f"âœ… .env æ–‡ä»¶è·¯å¾„: {env_path}")

    # è¯»å–å…³é”®é…ç½®
    configs = {}
    with open(env_path) as f:
        for line in f:
            line = line.strip()
            if line.startswith("EMBEDDING_") or line.startswith("LITELLM_EMBEDDING_"):
                if "=" in line and not line.startswith("#"):
                    key, value = line.split("=", 1)
                    configs[key] = value

    print("\nğŸ”§ Embedding ç›¸å…³é…ç½®:")
    print(f"  EMBEDDING_OPENAI_API_KEY: {configs.get('EMBEDDING_OPENAI_API_KEY', 'æœªè®¾ç½®')}")
    print(f"  EMBEDDING_OPENAI_BASE_URL: {configs.get('EMBEDDING_OPENAI_BASE_URL', 'æœªè®¾ç½®')}")
    print(f"  LITELLM_EMBEDDING_MODEL: {configs.get('LITELLM_EMBEDDING_MODEL', 'æœªè®¾ç½®')}")
    print(f"  prompt_cache_path: {configs.get('prompt_cache_path', 'æœªè®¾ç½®')}")

    # éªŒè¯é…ç½®
    print("\nâœ… é…ç½®éªŒè¯:")
    checks = []

    # æ£€æŸ¥ API Key
    if configs.get('EMBEDDING_OPENAI_API_KEY') == 'ollama':
        print("  âœ… API Key è®¾ç½®æ­£ç¡® (ollama)")
        checks.append(True)
    else:
        print("  âŒ API Key åº”è®¾ç½®ä¸º 'ollama'")
        checks.append(False)

    # æ£€æŸ¥ Base URL
    if 'localhost:11434' in configs.get('EMBEDDING_OPENAI_BASE_URL', ''):
        print("  âœ… Base URL æŒ‡å‘æœ¬åœ° Ollama")
        checks.append(True)
    else:
        print("  âŒ Base URL åº”æŒ‡å‘ http://localhost:11434")
        checks.append(False)

    # æ£€æŸ¥æ¨¡å‹
    if 'bge-m3' in configs.get('LITELLM_EMBEDDING_MODEL', '').lower():
        print("  âœ… æ¨¡å‹è®¾ç½®ä¸º BGE-M3")
        checks.append(True)
    else:
        print("  âš ï¸  æ¨¡å‹æœªè®¾ç½®ä¸º bge-m3")
        checks.append(False)

    return all(checks)

def check_llm_conf():
    """æ£€æŸ¥ llm_conf.py é…ç½®"""
    print("\n" + "=" * 60)
    print("ğŸ”§ llm_conf.py é…ç½®æ£€æŸ¥")
    print("=" * 60)

    conf_path = Path.cwd() / "rdagent" / "oai" / "llm_conf.py"
    if not conf_path.exists():
        print("âŒ llm_conf.py æ–‡ä»¶ä¸å­˜åœ¨")
        return False

    print(f"âœ… é…ç½®æ–‡ä»¶: {conf_path}")

    # è¯»å–é…ç½®æ–‡ä»¶
    with open(conf_path) as f:
        content = f.read()

    # æ£€æŸ¥å…³é”®é…ç½®
    print("\nğŸ” å…³é”®é…ç½®æ£€æŸ¥:")

    checks = []

    if 'embedding_model: str = "ollama/bge-m3"' in content:
        print("  âœ… embedding_model å·²è®¾ç½®ä¸º ollama/bge-m3")
        checks.append(True)
    else:
        print("  âŒ embedding_model æœªæ­£ç¡®è®¾ç½®")
        checks.append(False)

    if 'embedding_max_str_num: int = 100' in content:
        print("  âœ… æ‰¹é‡å¤§å°å·²ä¼˜åŒ–ä¸º 100")
        checks.append(True)
    else:
        print("  âš ï¸  æ‰¹é‡å¤§å°æœªä¼˜åŒ–")
        checks.append(False)

    if 'use_embedding_cache: bool = True' in content:
        print("  âœ… Embedding ç¼“å­˜å·²å¯ç”¨")
        checks.append(True)
    else:
        print("  âŒ Embedding ç¼“å­˜æœªå¯ç”¨")
        checks.append(False)

    if 'dump_embedding_cache: bool = True' in content:
        print("  âœ… ç¼“å­˜è½¬å‚¨å·²å¯ç”¨")
        checks.append(True)
    else:
        print("  âš ï¸  ç¼“å­˜è½¬å‚¨æœªå¯ç”¨")
        checks.append(False)

    if 'embedding_openai_base_url: str = "http://localhost:11434"' in content:
        print("  âœ… Base URL é…ç½®æ­£ç¡®")
        checks.append(True)
    else:
        print("  âŒ Base URL æœªé…ç½®")
        checks.append(False)

    return all(checks)

def check_ollama_service():
    """æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€"""
    print("\n" + "=" * 60)
    print("ğŸ”„ Ollama æœåŠ¡æ£€æŸ¥")
    print("=" * 60)

    import subprocess

    # æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ
    try:
        result = subprocess.run(
            ["pgrep", "-x", "ollama"],
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            print("âœ… Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ")
        else:
            print("âŒ Ollama æœåŠ¡æœªè¿è¡Œ")
            print("   è¯·æ‰§è¡Œ: ollama serve")
            return False
    except:
        print("âš ï¸  æ— æ³•æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€")

    # æ£€æŸ¥ BGE-M3 æ¨¡å‹
    try:
        result = subprocess.run(
            ["ollama", "list"],
            capture_output=True,
            text=True
        )
        if "bge-m3" in result.stdout:
            print("âœ… BGE-M3 æ¨¡å‹å·²å®‰è£…")

            # æå–æ¨¡å‹å¤§å°
            for line in result.stdout.split('\n'):
                if 'bge-m3' in line:
                    print(f"   {line.strip()}")
        else:
            print("âŒ BGE-M3 æ¨¡å‹æœªå®‰è£…")
            print("   è¯·æ‰§è¡Œ: ollama pull bge-m3")
            return False
    except:
        print("âš ï¸  æ— æ³•æ£€æŸ¥æ¨¡å‹åˆ—è¡¨")

    # æµ‹è¯• API
    print("\nğŸ§ª æµ‹è¯• Embedding API...")
    try:
        import json
        result = subprocess.run(
            ['curl', '-s', 'http://localhost:11434/api/embeddings',
             '-d', '{"model": "bge-m3", "prompt": "æµ‹è¯•"}'],
            capture_output=True,
            text=True,
            timeout=10
        )

        if result.returncode == 0:
            response = json.loads(result.stdout)
            if 'embedding' in response:
                embedding = response['embedding']
                print(f"âœ… API æµ‹è¯•æˆåŠŸ")
                print(f"   å‘é‡ç»´åº¦: {len(embedding)}")
                return True
            else:
                print("âŒ API å“åº”æ ¼å¼é”™è¯¯")
                return False
        else:
            print("âŒ API è¯·æ±‚å¤±è´¥")
            return False
    except Exception as e:
        print(f"âŒ API æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def check_cache_dir():
    """æ£€æŸ¥ç¼“å­˜ç›®å½•"""
    print("\n" + "=" * 60)
    print("ğŸ’¾ ç¼“å­˜ç›®å½•æ£€æŸ¥")
    print("=" * 60)

    cache_path = Path.cwd() / "ollama_cache"

    if not cache_path.exists():
        print(f"âš ï¸  ç¼“å­˜ç›®å½•ä¸å­˜åœ¨: {cache_path}")
        print(f"   æ­£åœ¨åˆ›å»º...")
        try:
            cache_path.mkdir(parents=True, exist_ok=True)
            print(f"âœ… ç¼“å­˜ç›®å½•å·²åˆ›å»º: {cache_path}")
        except Exception as e:
            print(f"âŒ åˆ›å»ºç¼“å­˜ç›®å½•å¤±è´¥: {e}")
            return False
    else:
        print(f"âœ… ç¼“å­˜ç›®å½•å­˜åœ¨: {cache_path}")

    # æ£€æŸ¥å†™å…¥æƒé™
    test_file = cache_path / ".test"
    try:
        test_file.touch()
        test_file.unlink()
        print("âœ… ç¼“å­˜ç›®å½•å¯å†™")
        return True
    except:
        print("âŒ ç¼“å­˜ç›®å½•ä¸å¯å†™")
        return False

def main():
    print("\n" + "=" * 60)
    print("ğŸš€ RD-Agent Ollama BGE-M3 é…ç½®éªŒè¯")
    print("=" * 60)
    print(f"ğŸ“ å·¥ä½œç›®å½•: {Path.cwd()}")
    print(f"ğŸ–¥ï¸  ç³»ç»Ÿ: {sys.platform}")
    print("=" * 60)

    results = {}

    # è¿è¡Œæ‰€æœ‰æ£€æŸ¥
    results['env'] = check_env_file()
    results['conf'] = check_llm_conf()
    results['cache'] = check_cache_dir()
    results['service'] = check_ollama_service()

    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š éªŒè¯ç»“æœæ€»ç»“")
    print("=" * 60)

    print(f"\n  .env é…ç½®:      {'âœ… é€šè¿‡' if results['env'] else 'âŒ å¤±è´¥'}")
    print(f"  llm_conf.py:    {'âœ… é€šè¿‡' if results['conf'] else 'âŒ å¤±è´¥'}")
    print(f"  ç¼“å­˜ç›®å½•:       {'âœ… é€šè¿‡' if results['cache'] else 'âŒ å¤±è´¥'}")
    print(f"  Ollama æœåŠ¡:    {'âœ… é€šè¿‡' if results['service'] else 'âŒ å¤±è´¥'}")

    all_passed = all(results.values())

    print("\n" + "=" * 60)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼é…ç½®å®Œæˆï¼")
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("   1. è¿è¡Œ RD-Agent ä»»åŠ¡")
        print("   2. BGE-M3 å°†è‡ªåŠ¨ç”¨äº embedding")
        print("   3. ç¼“å­˜å°†è‡ªåŠ¨ä¿å­˜åˆ° ./ollama_cache/")
    else:
        print("âš ï¸  éƒ¨åˆ†æ£€æŸ¥å¤±è´¥ï¼Œè¯·æ ¹æ®ä¸Šè¿°æç¤ºä¿®å¤")

    print("=" * 60 + "\n")

    return 0 if all_passed else 1

if __name__ == "__main__":
    sys.exit(main())
